[0m20:40:32.394216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDACC6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDB7C1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDCE87ED0>]}


============================== 20:40:32.424096 | 620a79f3-ed18-4c3d-b0ad-262eea3d156c ==============================
[0m20:40:32.424096 [info ] [MainThread]: Running with dbt=1.10.13
[0m20:40:32.425692 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'target_path': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m20:40:33.709421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '620a79f3-ed18-4c3d-b0ad-262eea3d156c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDB1BE190>]}
[0m20:40:33.915898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '620a79f3-ed18-4c3d-b0ad-262eea3d156c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDCD92CF0>]}
[0m20:40:33.929734 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m20:40:34.703089 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m20:40:35.048056 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:40:35.049370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '620a79f3-ed18-4c3d-b0ad-262eea3d156c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDCF4A050>]}
[0m20:40:39.732779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '620a79f3-ed18-4c3d-b0ad-262eea3d156c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDD75A120>]}
[0m20:40:40.062304 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m20:40:40.069796 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m20:40:40.391847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '620a79f3-ed18-4c3d-b0ad-262eea3d156c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDD8235B0>]}
[0m20:40:40.393513 [info ] [MainThread]: Found 9 models, 17 data tests, 485 macros
[0m20:40:40.395098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '620a79f3-ed18-4c3d-b0ad-262eea3d156c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDD5A6340>]}
[0m20:40:40.404153 [info ] [MainThread]: 
[0m20:40:40.405710 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:40:40.406902 [info ] [MainThread]: 
[0m20:40:40.408706 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m20:40:40.442751 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m20:40:40.476160 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:53.296050 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000023FE00F42D0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m20:40:59.681540 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m20:40:59.683904 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:40:59.685155 [debug] [MainThread]: Connection 'list_' was left open.
[0m20:40:59.686326 [debug] [MainThread]: On list_: No close available on handle
[0m20:40:59.687406 [info ] [MainThread]: 
[0m20:40:59.688723 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 19.28 seconds (19.28s).
[0m20:40:59.690211 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000023FE003F6F0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m20:40:59.692962 [debug] [MainThread]: Command `dbt run` failed at 20:40:59.692611 after 27.66 seconds
[0m20:40:59.694017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FDCC96810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FE00CD2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FE00CD230>]}
[0m20:40:59.695098 [debug] [MainThread]: Flushing usage events
[0m20:41:00.263756 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:41:23.286153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6B9B6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6C4B1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6DB77ED0>]}


============================== 20:41:23.299042 | 7a08ec1e-c202-4f4b-8b76-210da35b5297 ==============================
[0m20:41:23.299042 [info ] [MainThread]: Running with dbt=1.10.13
[0m20:41:23.301289 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'printer_width': '80', 'invocation_command': 'dbt test', 'write_json': 'True', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m20:41:24.189176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a08ec1e-c202-4f4b-8b76-210da35b5297', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6BEAE190>]}
[0m20:41:24.407877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a08ec1e-c202-4f4b-8b76-210da35b5297', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6DA83680>]}
[0m20:41:24.410036 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m20:41:25.174918 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m20:41:26.169081 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:41:26.170359 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:41:26.349539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a08ec1e-c202-4f4b-8b76-210da35b5297', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6E1C8D50>]}
[0m20:41:26.632471 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m20:41:26.639569 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m20:41:26.934854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a08ec1e-c202-4f4b-8b76-210da35b5297', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6E2E64E0>]}
[0m20:41:26.936354 [info ] [MainThread]: Found 9 models, 17 data tests, 485 macros
[0m20:41:26.937642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a08ec1e-c202-4f4b-8b76-210da35b5297', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6DEF4BB0>]}
[0m20:41:26.943695 [info ] [MainThread]: 
[0m20:41:26.944792 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:41:26.945571 [info ] [MainThread]: 
[0m20:41:26.946627 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m20:41:26.965420 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m20:41:27.007859 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:41:34.057405 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000019B70B65450>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m20:41:40.453506 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m20:41:40.455873 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:41:40.456955 [debug] [MainThread]: Connection 'list__default' was left open.
[0m20:41:40.458064 [debug] [MainThread]: On list__default: No close available on handle
[0m20:41:40.459012 [info ] [MainThread]: 
[0m20:41:40.460245 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.51 seconds (13.51s).
[0m20:41:40.461881 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000019B70BD4D60>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m20:41:40.464843 [debug] [MainThread]: Command `dbt test` failed at 20:41:40.464463 after 17.70 seconds
[0m20:41:40.465939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B70BAD230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6C9B6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B6E2822D0>]}
[0m20:41:40.467093 [debug] [MainThread]: Flushing usage events
[0m20:41:40.994458 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:41:50.789335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B1A66660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B2561F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B3C27ED0>]}


============================== 20:41:50.801668 | 8840ca6d-e17b-434f-96cd-f63399c2894f ==============================
[0m20:41:50.801668 [info ] [MainThread]: Running with dbt=1.10.13
[0m20:41:50.803154 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'cache_selected_only': 'False', 'write_json': 'True', 'no_print': 'None', 'warn_error': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'printer_width': '80', 'target_path': 'None'}
[0m20:41:51.537151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8840ca6d-e17b-434f-96cd-f63399c2894f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B1F5E190>]}
[0m20:41:51.724476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8840ca6d-e17b-434f-96cd-f63399c2894f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B3B32CF0>]}
[0m20:41:51.727173 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m20:41:52.384645 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m20:41:52.859074 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:41:52.860081 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:41:53.019147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8840ca6d-e17b-434f-96cd-f63399c2894f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B42C0E50>]}
[0m20:41:53.262092 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m20:41:53.268284 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m20:41:53.337484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8840ca6d-e17b-434f-96cd-f63399c2894f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B1A5E030>]}
[0m20:41:53.338493 [info ] [MainThread]: Found 9 models, 17 data tests, 485 macros
[0m20:41:53.339582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8840ca6d-e17b-434f-96cd-f63399c2894f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B3FE73F0>]}
[0m20:41:53.344962 [info ] [MainThread]: 
[0m20:41:53.346621 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:41:53.347632 [info ] [MainThread]: 
[0m20:41:53.349517 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m20:41:53.372435 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m20:41:53.407229 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:42:00.380768 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001E4B6AFA0D0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m20:42:06.779855 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m20:42:06.782558 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:42:06.783633 [debug] [MainThread]: Connection 'list_' was left open.
[0m20:42:06.784580 [debug] [MainThread]: On list_: No close available on handle
[0m20:42:06.785372 [info ] [MainThread]: 
[0m20:42:06.786466 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.44 seconds (13.44s).
[0m20:42:06.787810 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001E4B6B55480>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m20:42:06.789911 [debug] [MainThread]: Command `dbt run` failed at 20:42:06.789693 after 16.34 seconds
[0m20:42:06.790659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B6B51160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B3A36810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4B43EC290>]}
[0m20:42:06.791584 [debug] [MainThread]: Flushing usage events
[0m20:42:07.372176 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:27:15.162471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011018D46660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011019841F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101AF07ED0>]}


============================== 11:27:15.187928 | 51f26829-b206-4e59-8510-6384cca397ce ==============================
[0m11:27:15.187928 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:27:15.189235 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'no_print': 'None', 'use_colors': 'True', 'log_format': 'default', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m11:27:15.967569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '51f26829-b206-4e59-8510-6384cca397ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101923E190>]}
[0m11:27:16.107458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '51f26829-b206-4e59-8510-6384cca397ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101AE12CF0>]}
[0m11:27:16.114844 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:27:16.688897 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:27:16.985198 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m11:27:16.986045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '51f26829-b206-4e59-8510-6384cca397ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101AFCA050>]}
[0m11:27:19.613059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51f26829-b206-4e59-8510-6384cca397ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101B76E210>]}
[0m11:27:19.759132 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:27:19.763862 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:27:19.992506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51f26829-b206-4e59-8510-6384cca397ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101B838E50>]}
[0m11:27:19.993456 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:27:19.994224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51f26829-b206-4e59-8510-6384cca397ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101B5D5300>]}
[0m11:27:19.998496 [info ] [MainThread]: 
[0m11:27:19.999356 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:27:20.000008 [info ] [MainThread]: 
[0m11:27:20.000887 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:27:20.016399 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m11:27:20.037195 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:27:31.297424 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001101E170A50>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:27:37.696924 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m11:27:37.698953 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:27:37.699504 [debug] [MainThread]: Connection 'list_' was left open.
[0m11:27:37.699972 [debug] [MainThread]: On list_: No close available on handle
[0m11:27:37.700404 [info ] [MainThread]: 
[0m11:27:37.701548 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 17.70 seconds (17.70s).
[0m11:27:37.702802 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001101E0D76F0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:27:37.704540 [debug] [MainThread]: Command `dbt run` failed at 11:27:37.704278 after 22.85 seconds
[0m11:27:37.705221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011019D46810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101E161230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001101E161180>]}
[0m11:27:37.705751 [debug] [MainThread]: Flushing usage events
[0m11:27:38.246061 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:27:47.636710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025785506660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025786001F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257876C7ED0>]}


============================== 11:27:47.643580 | 580a694e-092f-4368-94f9-f7d13c5b75f3 ==============================
[0m11:27:47.643580 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:27:47.644693 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'printer_width': '80', 'write_json': 'True', 'invocation_command': 'dbt test', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m11:27:48.416242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '580a694e-092f-4368-94f9-f7d13c5b75f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257859FE190>]}
[0m11:27:48.587664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '580a694e-092f-4368-94f9-f7d13c5b75f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257875D3680>]}
[0m11:27:48.589694 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:27:49.156162 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:27:49.523778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:27:49.524596 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:27:49.640207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '580a694e-092f-4368-94f9-f7d13c5b75f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025787C69150>]}
[0m11:27:49.832173 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:27:49.836469 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:27:50.027702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '580a694e-092f-4368-94f9-f7d13c5b75f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025787DAA6C0>]}
[0m11:27:50.028935 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:27:50.030241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '580a694e-092f-4368-94f9-f7d13c5b75f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257879B4BB0>]}
[0m11:27:50.036331 [info ] [MainThread]: 
[0m11:27:50.037278 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:27:50.038025 [info ] [MainThread]: 
[0m11:27:50.039074 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:27:50.057493 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m11:27:50.083900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:27:56.762970 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002578A6A1BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:28:03.161791 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m11:28:03.163345 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:28:03.163981 [debug] [MainThread]: Connection 'list__default' was left open.
[0m11:28:03.164508 [debug] [MainThread]: On list__default: No close available on handle
[0m11:28:03.165020 [info ] [MainThread]: 
[0m11:28:03.165839 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.13 seconds (13.13s).
[0m11:28:03.166826 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002578A72CD60>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:28:03.168838 [debug] [MainThread]: Command `dbt test` failed at 11:28:03.168633 after 15.78 seconds
[0m11:28:03.169471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002578A701300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257874D6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025787D3E2D0>]}
[0m11:28:03.170097 [debug] [MainThread]: Flushing usage events
[0m11:28:03.642570 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:45:03.789715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9548C6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9553C1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D956A87ED0>]}


============================== 11:45:03.801593 | ac7be73d-007c-44e3-b721-eab9fbfbb738 ==============================
[0m11:45:03.801593 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:45:03.803236 [debug] [MainThread]: running dbt with arguments {'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'log_format': 'default', 'target_path': 'None', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m11:45:04.467208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac7be73d-007c-44e3-b721-eab9fbfbb738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D954DBE190>]}
[0m11:45:04.604452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ac7be73d-007c-44e3-b721-eab9fbfbb738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D956992CF0>]}
[0m11:45:04.606293 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:45:05.136877 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:45:05.525223 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:45:05.526027 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:45:05.628392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac7be73d-007c-44e3-b721-eab9fbfbb738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D957029050>]}
[0m11:45:05.819261 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:45:05.824268 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:45:05.880668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac7be73d-007c-44e3-b721-eab9fbfbb738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9548BE030>]}
[0m11:45:05.881818 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:45:05.883038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac7be73d-007c-44e3-b721-eab9fbfbb738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D956D773F0>]}
[0m11:45:05.888774 [info ] [MainThread]: 
[0m11:45:05.889654 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:45:05.890607 [info ] [MainThread]: 
[0m11:45:05.892771 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:45:05.907913 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m11:45:05.936041 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:13.004471 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001D959936850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:45:19.401053 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m11:45:19.403102 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:45:19.403836 [debug] [MainThread]: Connection 'list_' was left open.
[0m11:45:19.404658 [debug] [MainThread]: On list_: No close available on handle
[0m11:45:19.405354 [info ] [MainThread]: 
[0m11:45:19.406199 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.51 seconds (13.51s).
[0m11:45:19.407694 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001D9599CD480>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:45:19.410184 [debug] [MainThread]: Command `dbt run` failed at 11:45:19.409905 after 16.02 seconds
[0m11:45:19.410970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9599C9160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9558C6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D957180290>]}
[0m11:45:19.411866 [debug] [MainThread]: Flushing usage events
[0m11:45:19.925350 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:45:39.950728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8B886660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8C37DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8DA47ED0>]}


============================== 11:45:39.958779 | 593af1c8-0830-486e-9588-37a1ac9ce5c8 ==============================
[0m11:45:39.958779 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:45:39.960255 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'cache_selected_only': 'False', 'use_colors': 'True', 'target_path': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run', 'printer_width': '80', 'write_json': 'True'}
[0m11:45:40.579466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '593af1c8-0830-486e-9588-37a1ac9ce5c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8BD7E190>]}
[0m11:45:40.725843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '593af1c8-0830-486e-9588-37a1ac9ce5c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8D952CF0>]}
[0m11:45:40.728484 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:45:41.290911 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:45:41.699356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:45:41.699980 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:45:41.816794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '593af1c8-0830-486e-9588-37a1ac9ce5c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8E089050>]}
[0m11:45:42.055592 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:45:42.061052 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:45:42.123158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '593af1c8-0830-486e-9588-37a1ac9ce5c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8B87E030>]}
[0m11:45:42.124337 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:45:42.125681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '593af1c8-0830-486e-9588-37a1ac9ce5c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8DDD73F0>]}
[0m11:45:42.132014 [info ] [MainThread]: 
[0m11:45:42.133354 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:45:42.134708 [info ] [MainThread]: 
[0m11:45:42.136536 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:45:42.159486 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m11:45:42.188710 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:49.065759 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000024B908FA850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:45:55.436656 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m11:45:55.438038 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:45:55.438518 [debug] [MainThread]: Connection 'list_' was left open.
[0m11:45:55.439106 [debug] [MainThread]: On list_: No close available on handle
[0m11:45:55.439579 [info ] [MainThread]: 
[0m11:45:55.440310 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.30 seconds (13.30s).
[0m11:45:55.441230 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000024B90991480>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:45:55.442865 [debug] [MainThread]: Command `dbt run` failed at 11:45:55.442672 after 15.77 seconds
[0m11:45:55.443458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B9098D160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8D856810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B8E1DC290>]}
[0m11:45:55.444029 [debug] [MainThread]: Flushing usage events
[0m11:45:55.934418 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:46:03.656569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A2386660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A2E7DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4547ED0>]}


============================== 11:46:03.666825 | b7409f1c-823e-4877-b787-694ff5fb03a0 ==============================
[0m11:46:03.666825 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:46:03.668495 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test', 'write_json': 'True', 'printer_width': '80', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'log_format': 'default', 'warn_error': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m11:46:04.358070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b7409f1c-823e-4877-b787-694ff5fb03a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A287E190>]}
[0m11:46:04.536504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b7409f1c-823e-4877-b787-694ff5fb03a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4453680>]}
[0m11:46:04.539073 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:46:05.099709 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:46:05.528883 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:46:05.529546 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:46:05.629176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b7409f1c-823e-4877-b787-694ff5fb03a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4B19150>]}
[0m11:46:05.887306 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:46:05.894487 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:46:06.079997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b7409f1c-823e-4877-b787-694ff5fb03a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4C566C0>]}
[0m11:46:06.080973 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:46:06.081983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b7409f1c-823e-4877-b787-694ff5fb03a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4864BB0>]}
[0m11:46:06.087997 [info ] [MainThread]: 
[0m11:46:06.089191 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:46:06.090358 [info ] [MainThread]: 
[0m11:46:06.092060 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:46:06.117703 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m11:46:06.148680 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:12.977080 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000119A7515BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:46:19.368280 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m11:46:19.369687 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:46:19.370234 [debug] [MainThread]: Connection 'list__default' was left open.
[0m11:46:19.370691 [debug] [MainThread]: On list__default: No close available on handle
[0m11:46:19.371125 [info ] [MainThread]: 
[0m11:46:19.372033 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.28 seconds (13.28s).
[0m11:46:19.373087 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000119A759CD60>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:46:19.375116 [debug] [MainThread]: Command `dbt test` failed at 11:46:19.374807 after 16.07 seconds
[0m11:46:19.375798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A7571300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4356810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000119A4BEE2D0>]}
[0m11:46:19.376445 [debug] [MainThread]: Flushing usage events
[0m11:46:19.849160 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:49:08.367573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED28256660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED28D51F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2A417ED0>]}


============================== 11:49:08.376579 | dbe277ee-60bd-4b93-88d6-6f1844862c2a ==============================
[0m11:49:08.376579 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:49:08.378268 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'printer_width': '80', 'write_json': 'True'}
[0m11:49:09.064117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dbe277ee-60bd-4b93-88d6-6f1844862c2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2874E190>]}
[0m11:49:09.248790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dbe277ee-60bd-4b93-88d6-6f1844862c2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2A322CF0>]}
[0m11:49:09.250768 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:49:09.861989 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:49:10.336506 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:49:10.337161 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:49:10.441247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dbe277ee-60bd-4b93-88d6-6f1844862c2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2AA09050>]}
[0m11:49:10.652176 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:49:10.657993 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:49:10.702268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dbe277ee-60bd-4b93-88d6-6f1844862c2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2824E030>]}
[0m11:49:10.703180 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:49:10.704133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dbe277ee-60bd-4b93-88d6-6f1844862c2a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2A7573F0>]}
[0m11:49:10.708695 [info ] [MainThread]: 
[0m11:49:10.709691 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:49:10.710463 [info ] [MainThread]: 
[0m11:49:10.711533 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:49:10.727886 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m11:49:10.757554 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:17.435382 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001ED2D2CA850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:49:23.833171 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m11:49:23.834525 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:49:23.835085 [debug] [MainThread]: Connection 'list_' was left open.
[0m11:49:23.835549 [debug] [MainThread]: On list_: No close available on handle
[0m11:49:23.835970 [info ] [MainThread]: 
[0m11:49:23.836772 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.12 seconds (13.12s).
[0m11:49:23.837662 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001ED2D35D480>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:49:23.839159 [debug] [MainThread]: Command `dbt run` failed at 11:49:23.838991 after 15.92 seconds
[0m11:49:23.839664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2D359160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2A226810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED2AB58290>]}
[0m11:49:23.840176 [debug] [MainThread]: Flushing usage events
[0m11:49:24.391847 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:49:31.481862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4D676660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4E171F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4F837ED0>]}


============================== 11:49:31.488276 | 09360b86-5ce6-4569-aab5-27e6e2a4c7f0 ==============================
[0m11:49:31.488276 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:49:31.489349 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'cache_selected_only': 'False', 'target_path': 'None', 'invocation_command': 'dbt test', 'no_print': 'None', 'warn_error': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m11:49:32.043170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '09360b86-5ce6-4569-aab5-27e6e2a4c7f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4DB6E190>]}
[0m11:49:32.235534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '09360b86-5ce6-4569-aab5-27e6e2a4c7f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4F743680>]}
[0m11:49:32.237908 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:49:32.918716 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:49:33.556183 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:49:33.557254 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:49:33.716270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09360b86-5ce6-4569-aab5-27e6e2a4c7f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4FE29150>]}
[0m11:49:34.009986 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:49:34.017732 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:49:34.206917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09360b86-5ce6-4569-aab5-27e6e2a4c7f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4FF666C0>]}
[0m11:49:34.208401 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:49:34.209758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09360b86-5ce6-4569-aab5-27e6e2a4c7f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4FB74BB0>]}
[0m11:49:34.216963 [info ] [MainThread]: 
[0m11:49:34.218430 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:49:34.219385 [info ] [MainThread]: 
[0m11:49:34.220958 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:49:34.248202 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m11:49:34.285743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:40.988901 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000020C52815BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:49:47.384624 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m11:49:47.386001 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:49:47.386500 [debug] [MainThread]: Connection 'list__default' was left open.
[0m11:49:47.386918 [debug] [MainThread]: On list__default: No close available on handle
[0m11:49:47.387313 [info ] [MainThread]: 
[0m11:49:47.388124 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.17 seconds (13.17s).
[0m11:49:47.389188 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000020C5289CD60>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:49:47.391133 [debug] [MainThread]: Command `dbt test` failed at 11:49:47.390821 after 16.14 seconds
[0m11:49:47.391826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C52871300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4F646810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4FF022D0>]}
[0m11:49:47.392515 [debug] [MainThread]: Flushing usage events
[0m11:49:47.878087 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:51:45.841199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA8086660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA8B81F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAA247ED0>]}


============================== 11:51:45.850723 | a2a9b6e4-6f4b-4c6b-8fa8-91e39e4b14da ==============================
[0m11:51:45.850723 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:51:45.852175 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'cache_selected_only': 'False', 'no_print': 'None', 'target_path': 'None', 'invocation_command': 'dbt run', 'warn_error': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m11:51:47.241347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2a9b6e4-6f4b-4c6b-8fa8-91e39e4b14da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA857E190>]}
[0m11:51:47.425003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2a9b6e4-6f4b-4c6b-8fa8-91e39e4b14da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAA152CF0>]}
[0m11:51:47.427109 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m11:51:47.988146 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:51:48.460586 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:51:48.461853 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:51:48.583971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2a9b6e4-6f4b-4c6b-8fa8-91e39e4b14da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAA7E9050>]}
[0m11:51:48.833959 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m11:51:48.838318 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m11:51:48.889447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2a9b6e4-6f4b-4c6b-8fa8-91e39e4b14da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA807E030>]}
[0m11:51:48.890792 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m11:51:48.892051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2a9b6e4-6f4b-4c6b-8fa8-91e39e4b14da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAA5373F0>]}
[0m11:51:48.897018 [info ] [MainThread]: 
[0m11:51:48.898023 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:51:48.898777 [info ] [MainThread]: 
[0m11:51:48.899902 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m11:51:48.918988 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m11:51:48.946167 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:55.890940 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000024BAD0E6850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:52:02.298562 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m11:52:02.301278 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:52:02.302039 [debug] [MainThread]: Connection 'list_' was left open.
[0m11:52:02.302996 [debug] [MainThread]: On list_: No close available on handle
[0m11:52:02.304241 [info ] [MainThread]: 
[0m11:52:02.306373 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.40 seconds (13.40s).
[0m11:52:02.308593 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000024BAD17D480>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m11:52:02.311918 [debug] [MainThread]: Command `dbt run` failed at 11:52:02.311339 after 16.77 seconds
[0m11:52:02.313791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAD179160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAA056810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BAA93C290>]}
[0m11:52:02.315692 [debug] [MainThread]: Flushing usage events
[0m11:52:02.816233 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:03:36.381401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A04B06660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A05601F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A06CC7ED0>]}


============================== 12:03:36.388286 | 59b64d01-05fd-4c38-a642-dc81be38019c ==============================
[0m12:03:36.388286 [info ] [MainThread]: Running with dbt=1.10.13
[0m12:03:36.389343 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'target_path': 'None', 'use_colors': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'printer_width': '80', 'write_json': 'True'}
[0m12:03:36.876212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59b64d01-05fd-4c38-a642-dc81be38019c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A04FFE190>]}
[0m12:03:37.018438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59b64d01-05fd-4c38-a642-dc81be38019c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A06BD2CF0>]}
[0m12:03:37.020079 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m12:03:37.471608 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m12:03:37.859308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:03:37.860806 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_stop_point.sql
[0m12:03:38.584453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59b64d01-05fd-4c38-a642-dc81be38019c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A0736D550>]}
[0m12:03:38.732594 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m12:03:38.737014 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m12:03:38.778635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59b64d01-05fd-4c38-a642-dc81be38019c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A04AFE030>]}
[0m12:03:38.779368 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m12:03:38.780214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59b64d01-05fd-4c38-a642-dc81be38019c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A06FD2C10>]}
[0m12:03:38.785611 [info ] [MainThread]: 
[0m12:03:38.786749 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:03:38.787425 [info ] [MainThread]: 
[0m12:03:38.788350 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m12:03:38.802511 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m12:03:38.818125 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:03:45.254419 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000028A09C8DD10>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m12:03:51.661367 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m12:03:51.662819 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:03:51.663428 [debug] [MainThread]: Connection 'list_' was left open.
[0m12:03:51.664017 [debug] [MainThread]: On list_: No close available on handle
[0m12:03:51.664482 [info ] [MainThread]: 
[0m12:03:51.665183 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 12.88 seconds (12.88s).
[0m12:03:51.666046 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000028A09CA2520>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m12:03:51.667499 [debug] [MainThread]: Command `dbt run` failed at 12:03:51.667334 after 15.62 seconds
[0m12:03:51.668000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A09D08460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A05B06810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028A07500C50>]}
[0m12:03:51.668501 [debug] [MainThread]: Flushing usage events
[0m12:03:52.161841 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:03:58.837186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109A446660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109AF41F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109C607ED0>]}


============================== 12:03:58.848234 | 54066ba5-bdc1-4826-8fa5-35e2ea7046e8 ==============================
[0m12:03:58.848234 [info ] [MainThread]: Running with dbt=1.10.13
[0m12:03:58.850046 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'invocation_command': 'dbt test', 'write_json': 'True', 'no_print': 'None', 'printer_width': '80', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m12:03:59.507968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '54066ba5-bdc1-4826-8fa5-35e2ea7046e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109A93E190>]}
[0m12:03:59.667928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '54066ba5-bdc1-4826-8fa5-35e2ea7046e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109C513680>]}
[0m12:03:59.670445 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m12:04:00.240944 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m12:04:00.705818 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:04:00.706507 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:04:00.814870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54066ba5-bdc1-4826-8fa5-35e2ea7046e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109CCA9150>]}
[0m12:04:01.018847 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m12:04:01.023661 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m12:04:01.188657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54066ba5-bdc1-4826-8fa5-35e2ea7046e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109CDCA6C0>]}
[0m12:04:01.189879 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m12:04:01.190961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54066ba5-bdc1-4826-8fa5-35e2ea7046e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109C9D4BB0>]}
[0m12:04:01.198000 [info ] [MainThread]: 
[0m12:04:01.199138 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:04:01.199889 [info ] [MainThread]: 
[0m12:04:01.200889 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m12:04:01.219694 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m12:04:01.247082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:04:07.741329 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002109F5E1BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m12:04:14.146362 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m12:04:14.149548 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:04:14.150658 [debug] [MainThread]: Connection 'list__default' was left open.
[0m12:04:14.151657 [debug] [MainThread]: On list__default: No close available on handle
[0m12:04:14.152617 [info ] [MainThread]: 
[0m12:04:14.153956 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 12.95 seconds (12.95s).
[0m12:04:14.155904 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002109F670D60>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m12:04:14.161894 [debug] [MainThread]: Command `dbt test` failed at 12:04:14.161417 after 15.57 seconds
[0m12:04:14.163420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109F641300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109B446810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002109CD5E2D0>]}
[0m12:04:14.164633 [debug] [MainThread]: Flushing usage events
[0m12:04:14.682559 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:23:28.962316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239C7EC6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239C89C1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239CA087ED0>]}


============================== 14:23:28.974612 | c208970b-deae-4cac-a000-67289438fa28 ==============================
[0m14:23:28.974612 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:23:28.976162 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'log_format': 'default', 'no_print': 'None', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run', 'printer_width': '80', 'write_json': 'True'}
[0m14:23:29.757588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c208970b-deae-4cac-a000-67289438fa28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239C83BE190>]}
[0m14:23:29.919601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c208970b-deae-4cac-a000-67289438fa28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239C9F92CF0>]}
[0m14:23:29.921972 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:23:30.641723 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:23:31.155269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m14:23:31.156414 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\silver\silver_stop_places.sql
[0m14:23:31.157201 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\staging\stg_netex_stopplaces.sql
[0m14:23:31.157957 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_stop_point.sql
[0m14:23:31.939554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c208970b-deae-4cac-a000-67289438fa28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239CA6E9550>]}
[0m14:23:32.174341 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:23:32.182314 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:23:32.248466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c208970b-deae-4cac-a000-67289438fa28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239CA15FE30>]}
[0m14:23:32.249364 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:23:32.250389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c208970b-deae-4cac-a000-67289438fa28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239CA28EC10>]}
[0m14:23:32.255700 [info ] [MainThread]: 
[0m14:23:32.256907 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:23:32.257773 [info ] [MainThread]: 
[0m14:23:32.258900 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:23:32.283415 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:23:32.311202 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:23:39.752930 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000239CD061D10>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:23:46.146019 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:23:46.147794 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:23:46.148662 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:23:46.149472 [debug] [MainThread]: On list_: No close available on handle
[0m14:23:46.150085 [info ] [MainThread]: 
[0m14:23:46.151034 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.89 seconds (13.89s).
[0m14:23:46.152165 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000239CD05AB10>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:23:46.154179 [debug] [MainThread]: Command `dbt run` failed at 14:23:46.153957 after 17.57 seconds
[0m14:23:46.154979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239CD0E02C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239C8EC6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239CA84CB90>]}
[0m14:23:46.155795 [debug] [MainThread]: Flushing usage events
[0m14:23:46.711982 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:23:55.089575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016991C06660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016992701F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016993DC7ED0>]}


============================== 14:23:55.100184 | 2f801014-2ac3-478f-9810-53f0aaee7c52 ==============================
[0m14:23:55.100184 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:23:55.101996 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'no_print': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'invocation_command': 'dbt test', 'printer_width': '80', 'write_json': 'True'}
[0m14:23:55.952681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f801014-2ac3-478f-9810-53f0aaee7c52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169920EE190>]}
[0m14:23:56.148816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f801014-2ac3-478f-9810-53f0aaee7c52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016993CD3680>]}
[0m14:23:56.151315 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:23:56.877461 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:23:57.444823 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:23:57.445926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:23:57.559471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f801014-2ac3-478f-9810-53f0aaee7c52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169943FD150>]}
[0m14:23:57.762536 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:23:57.767936 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:23:57.920668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f801014-2ac3-478f-9810-53f0aaee7c52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169944DA6C0>]}
[0m14:23:57.921619 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:23:57.922598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f801014-2ac3-478f-9810-53f0aaee7c52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169940E4BB0>]}
[0m14:23:57.930588 [info ] [MainThread]: 
[0m14:23:57.932057 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:23:57.932990 [info ] [MainThread]: 
[0m14:23:57.934600 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:23:57.957175 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:23:57.988109 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:04.724289 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000016996D91BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:24:11.146632 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m14:24:11.148562 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:11.149476 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:24:11.150145 [debug] [MainThread]: On list__default: No close available on handle
[0m14:24:11.150764 [info ] [MainThread]: 
[0m14:24:11.151996 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.22 seconds (13.22s).
[0m14:24:11.153244 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000016996E18E90>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:24:11.155362 [debug] [MainThread]: Command `dbt test` failed at 14:24:11.155127 after 16.40 seconds
[0m14:24:11.156317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016996DF1300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016993BD6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169944722D0>]}
[0m14:24:11.157024 [debug] [MainThread]: Flushing usage events
[0m14:24:11.640341 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:55.402366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029403D46660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002940483DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029405F07ED0>]}


============================== 14:24:55.413979 | 1c1f7585-1b0a-41f9-8b69-53330419f59a ==============================
[0m14:24:55.413979 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:24:55.415297 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'log_format': 'default', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m14:24:56.059060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1c1f7585-1b0a-41f9-8b69-53330419f59a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002940423E190>]}
[0m14:24:56.203906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1c1f7585-1b0a-41f9-8b69-53330419f59a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029405E12CF0>]}
[0m14:24:56.205746 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:24:56.756323 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:24:57.130724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:24:57.131419 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:24:57.230742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c1f7585-1b0a-41f9-8b69-53330419f59a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029406551050>]}
[0m14:24:57.444847 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:24:57.449628 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:24:57.501080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c1f7585-1b0a-41f9-8b69-53330419f59a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029403D3E030>]}
[0m14:24:57.502143 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:24:57.503328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c1f7585-1b0a-41f9-8b69-53330419f59a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294062A73F0>]}
[0m14:24:57.509959 [info ] [MainThread]: 
[0m14:24:57.511145 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:57.511990 [info ] [MainThread]: 
[0m14:24:57.513136 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:24:57.535643 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:24:57.564515 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:04.527926 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000029408DB6850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:25:10.941550 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:25:10.943558 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:10.944282 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:25:10.944912 [debug] [MainThread]: On list_: No close available on handle
[0m14:25:10.945444 [info ] [MainThread]: 
[0m14:25:10.946383 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.43 seconds (13.43s).
[0m14:25:10.947669 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000029408E495B0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:25:10.949708 [debug] [MainThread]: Command `dbt run` failed at 14:25:10.949356 after 15.86 seconds
[0m14:25:10.950574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029408E4D160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029405D16810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294066A8290>]}
[0m14:25:10.951224 [debug] [MainThread]: Flushing usage events
[0m14:25:11.467872 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:20.942361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D35E3E6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D35EEE1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D3605A7ED0>]}


============================== 14:25:20.950000 | 3771d601-64f1-4515-af36-8f41214bd9ad ==============================
[0m14:25:20.950000 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:25:20.951445 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt test', 'cache_selected_only': 'False', 'no_print': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m14:25:21.601886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3771d601-64f1-4515-af36-8f41214bd9ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D35E8DE190>]}
[0m14:25:21.747271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3771d601-64f1-4515-af36-8f41214bd9ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D3604B3680>]}
[0m14:25:21.749179 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:25:22.309545 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:25:22.718833 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:22.719508 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:22.841072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3771d601-64f1-4515-af36-8f41214bd9ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D360B75150>]}
[0m14:25:23.072379 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:25:23.077669 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:25:23.237160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3771d601-64f1-4515-af36-8f41214bd9ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D360CAA6C0>]}
[0m14:25:23.238552 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:25:23.239857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3771d601-64f1-4515-af36-8f41214bd9ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D3608B4BB0>]}
[0m14:25:23.247034 [info ] [MainThread]: 
[0m14:25:23.248203 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:23.249275 [info ] [MainThread]: 
[0m14:25:23.250803 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:25:23.278312 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:25:23.312476 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:30.032778 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002D363581BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:25:36.468013 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m14:25:36.470428 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:36.471287 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:25:36.472446 [debug] [MainThread]: On list__default: No close available on handle
[0m14:25:36.473898 [info ] [MainThread]: 
[0m14:25:36.475297 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.22 seconds (13.22s).
[0m14:25:36.488580 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002D363608E90>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:25:36.492586 [debug] [MainThread]: Command `dbt test` failed at 14:25:36.491995 after 15.85 seconds
[0m14:25:36.493941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D3635E1300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D3603B6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D360C422D0>]}
[0m14:25:36.494844 [debug] [MainThread]: Flushing usage events
[0m14:25:37.058600 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:43.735962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BAF56660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BBA51F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BD117ED0>]}


============================== 14:27:43.747232 | adedf9be-15e6-4f1c-9435-a0d08fb284a1 ==============================
[0m14:27:43.747232 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:27:43.749052 [debug] [MainThread]: running dbt with arguments {'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'write_json': 'True', 'log_format': 'default', 'use_colors': 'True', 'printer_width': '80', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m14:27:44.566060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adedf9be-15e6-4f1c-9435-a0d08fb284a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BB44E190>]}
[0m14:27:44.764732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adedf9be-15e6-4f1c-9435-a0d08fb284a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BD022CF0>]}
[0m14:27:44.767391 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:27:45.384160 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:27:45.805689 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:27:45.806866 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\staging\stg_netex_stopplaces.sql
[0m14:27:46.544081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adedf9be-15e6-4f1c-9435-a0d08fb284a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BD6D1550>]}
[0m14:27:46.747627 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:27:46.752232 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:27:46.797928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adedf9be-15e6-4f1c-9435-a0d08fb284a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BAF4E030>]}
[0m14:27:46.799189 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:27:46.800200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adedf9be-15e6-4f1c-9435-a0d08fb284a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BD322C10>]}
[0m14:27:46.804882 [info ] [MainThread]: 
[0m14:27:46.805880 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:46.806944 [info ] [MainThread]: 
[0m14:27:46.808212 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:27:46.832803 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:27:46.864546 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:53.436639 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001E4C00E1D10>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:27:59.855668 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:27:59.857330 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:59.857923 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:27:59.858470 [debug] [MainThread]: On list_: No close available on handle
[0m14:27:59.858993 [info ] [MainThread]: 
[0m14:27:59.859760 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.05 seconds (13.05s).
[0m14:27:59.860712 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001E4C00C2D70>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:27:59.862371 [debug] [MainThread]: Command `dbt run` failed at 14:27:59.862191 after 16.55 seconds
[0m14:27:59.862953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4C0158530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BCF26810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BD774C50>]}
[0m14:27:59.863509 [debug] [MainThread]: Flushing usage events
[0m14:28:00.419594 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:28:08.220296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020478286660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020478D81F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047A447ED0>]}


============================== 14:28:08.229626 | b523832d-efcd-4c62-baa2-06a45727b26c ==============================
[0m14:28:08.229626 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:28:08.231769 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'use_colors': 'True', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'log_format': 'default', 'printer_width': '80', 'warn_error': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt test', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m14:28:09.172942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b523832d-efcd-4c62-baa2-06a45727b26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047877E190>]}
[0m14:28:09.408875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b523832d-efcd-4c62-baa2-06a45727b26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047A353680>]}
[0m14:28:09.411610 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:28:10.077231 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:28:10.535089 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:28:10.535817 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:28:10.659298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b523832d-efcd-4c62-baa2-06a45727b26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047AAA5150>]}
[0m14:28:10.877352 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:28:10.882695 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:28:11.035435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b523832d-efcd-4c62-baa2-06a45727b26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047ABDE6C0>]}
[0m14:28:11.036439 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:28:11.037541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b523832d-efcd-4c62-baa2-06a45727b26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047A7E4BB0>]}
[0m14:28:11.042980 [info ] [MainThread]: 
[0m14:28:11.043915 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:28:11.044811 [info ] [MainThread]: 
[0m14:28:11.045906 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:28:11.064238 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:28:11.092455 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:17.710867 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002047D415BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:28:24.136132 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m14:28:24.139245 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:28:24.140709 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:28:24.141874 [debug] [MainThread]: On list__default: No close available on handle
[0m14:28:24.144234 [info ] [MainThread]: 
[0m14:28:24.146078 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.10 seconds (13.10s).
[0m14:28:24.150365 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000002047D498E90>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:28:24.157426 [debug] [MainThread]: Command `dbt test` failed at 14:28:24.156893 after 16.26 seconds
[0m14:28:24.158988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047D471300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047A256810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002047AB722D0>]}
[0m14:28:24.160853 [debug] [MainThread]: Flushing usage events
[0m14:28:24.671110 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:01.362560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F76E496660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F76EF8DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F770657ED0>]}


============================== 14:31:01.376898 | d14dcb5c-c845-45a1-b29f-aabc58a32d5d ==============================
[0m14:31:01.376898 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:31:01.379488 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'cache_selected_only': 'False', 'no_print': 'None', 'use_colors': 'True', 'log_format': 'default', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run', 'printer_width': '80', 'write_json': 'True'}
[0m14:31:02.338959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd14dcb5c-c845-45a1-b29f-aabc58a32d5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F76E98E190>]}
[0m14:31:02.572661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd14dcb5c-c845-45a1-b29f-aabc58a32d5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F770562CF0>]}
[0m14:31:02.575230 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:31:03.362944 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:31:03.848566 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:31:03.850396 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\staging\stg_netex_stopplaces.sql
[0m14:31:04.609674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd14dcb5c-c845-45a1-b29f-aabc58a32d5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F770C45550>]}
[0m14:31:04.800746 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:31:04.805940 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:31:04.851129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd14dcb5c-c845-45a1-b29f-aabc58a32d5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F76E48E030>]}
[0m14:31:04.852029 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:31:04.852992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd14dcb5c-c845-45a1-b29f-aabc58a32d5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7708B2C10>]}
[0m14:31:04.857867 [info ] [MainThread]: 
[0m14:31:04.858956 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:31:04.859967 [info ] [MainThread]: 
[0m14:31:04.861427 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:31:04.879066 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:31:04.899141 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:11.475274 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001F773621D10>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:31:17.889087 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:31:17.890672 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:31:17.891324 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:31:17.891977 [debug] [MainThread]: On list_: No close available on handle
[0m14:31:17.892540 [info ] [MainThread]: 
[0m14:31:17.893481 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.03 seconds (13.03s).
[0m14:31:17.894551 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001F773602D70>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:31:17.896310 [debug] [MainThread]: Command `dbt run` failed at 14:31:17.896102 after 17.05 seconds
[0m14:31:17.896956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F773698530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F76F496810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F770DE0C50>]}
[0m14:31:17.897575 [debug] [MainThread]: Flushing usage events
[0m14:31:18.400741 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:26.844478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB34FF6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB35AF1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB371B7ED0>]}


============================== 14:31:26.853014 | c8bfaa0b-9b11-421b-a2c6-24dbb4a61ca7 ==============================
[0m14:31:26.853014 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:31:26.854193 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'target_path': 'None', 'no_print': 'None', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'invocation_command': 'dbt test', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m14:31:27.493806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8bfaa0b-9b11-421b-a2c6-24dbb4a61ca7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB354EE190>]}
[0m14:31:27.632716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8bfaa0b-9b11-421b-a2c6-24dbb4a61ca7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB370C3680>]}
[0m14:31:27.637484 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:31:28.261877 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:31:28.683531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:31:28.684412 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:31:28.796589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8bfaa0b-9b11-421b-a2c6-24dbb4a61ca7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB37815150>]}
[0m14:31:29.027912 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:31:29.034196 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:31:29.204406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8bfaa0b-9b11-421b-a2c6-24dbb4a61ca7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB3793A6C0>]}
[0m14:31:29.205399 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:31:29.206483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8bfaa0b-9b11-421b-a2c6-24dbb4a61ca7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB37544BB0>]}
[0m14:31:29.212124 [info ] [MainThread]: 
[0m14:31:29.213377 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:31:29.214463 [info ] [MainThread]: 
[0m14:31:29.215713 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:31:29.236878 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:31:29.272318 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:35.847940 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001DB3A185BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:31:42.243774 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m14:31:42.245565 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:31:42.246211 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:31:42.246838 [debug] [MainThread]: On list__default: No close available on handle
[0m14:31:42.247341 [info ] [MainThread]: 
[0m14:31:42.248122 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.03 seconds (13.03s).
[0m14:31:42.249181 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001DB3A208E90>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:31:42.251365 [debug] [MainThread]: Command `dbt test` failed at 14:31:42.250931 after 15.69 seconds
[0m14:31:42.252287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB3A1E1300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB36FC6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB378D22D0>]}
[0m14:31:42.253287 [debug] [MainThread]: Flushing usage events
[0m14:31:42.728898 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:40:12.856475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462B3F6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462BEF1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462D5B7ED0>]}


============================== 14:40:12.879321 | d4b4b678-a4b4-4e28-9f1b-40ad25b2653b ==============================
[0m14:40:12.879321 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:40:12.882396 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'cache_selected_only': 'False', 'target_path': 'None', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m14:40:14.716287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4b4b678-a4b4-4e28-9f1b-40ad25b2653b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462B8EE190>]}
[0m14:40:15.117617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4b4b678-a4b4-4e28-9f1b-40ad25b2653b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462D4C2CF0>]}
[0m14:40:15.122820 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:40:16.593143 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:40:17.674417 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:40:17.675985 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:40:17.897206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd4b4b678-a4b4-4e28-9f1b-40ad25b2653b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462DBF5050>]}
[0m14:40:18.300538 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:40:18.309888 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:40:18.416259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd4b4b678-a4b4-4e28-9f1b-40ad25b2653b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462B3EE030>]}
[0m14:40:18.418074 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:40:18.420326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4b4b678-a4b4-4e28-9f1b-40ad25b2653b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462D9373F0>]}
[0m14:40:18.430595 [info ] [MainThread]: 
[0m14:40:18.432355 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:40:18.433792 [info ] [MainThread]: 
[0m14:40:18.435564 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:40:18.470292 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:40:18.515846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:28.118552 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000024630466850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:40:34.512388 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:40:34.515995 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:40:34.517676 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:40:34.519317 [debug] [MainThread]: On list_: No close available on handle
[0m14:40:34.521131 [info ] [MainThread]: 
[0m14:40:34.523379 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 16.09 seconds (16.09s).
[0m14:40:34.525741 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000246304F95B0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:40:34.533024 [debug] [MainThread]: Command `dbt run` failed at 14:40:34.532347 after 22.51 seconds
[0m14:40:34.535284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000246304FD160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462C3F6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002462DD3C290>]}
[0m14:40:34.537333 [debug] [MainThread]: Flushing usage events
[0m14:40:35.072281 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:40:49.584635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169AF886660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B0381F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B1A47ED0>]}


============================== 14:40:49.601033 | da148446-9599-4af0-96a4-ad00960e21fe ==============================
[0m14:40:49.601033 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:40:49.603832 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'no_print': 'None', 'printer_width': '80', 'write_json': 'True', 'use_colors': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt test', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m14:40:51.401245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da148446-9599-4af0-96a4-ad00960e21fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169AFD7E190>]}
[0m14:40:51.795912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da148446-9599-4af0-96a4-ad00960e21fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B1953680>]}
[0m14:40:51.800244 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:40:52.931850 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:40:53.773286 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:40:53.774551 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:40:53.988318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da148446-9599-4af0-96a4-ad00960e21fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B2095150>]}
[0m14:40:54.401899 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:40:54.410692 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:40:54.704792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da148446-9599-4af0-96a4-ad00960e21fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B21CA6C0>]}
[0m14:40:54.706928 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:40:54.709517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da148446-9599-4af0-96a4-ad00960e21fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B1DD4BB0>]}
[0m14:40:54.723670 [info ] [MainThread]: 
[0m14:40:54.725918 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:40:54.727770 [info ] [MainThread]: 
[0m14:40:54.731049 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:40:54.774802 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:40:54.829734 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:41:02.221937 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000169B4A25BD0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:41:08.605227 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_relations_without_caching
[0m14:41:08.607241 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:41:08.608165 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:41:08.609093 [debug] [MainThread]: On list__default: No close available on handle
[0m14:41:08.609685 [info ] [MainThread]: 
[0m14:41:08.610667 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.88 seconds (13.88s).
[0m14:41:08.611853 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x00000169B4AA4E90>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:41:08.613895 [debug] [MainThread]: Command `dbt test` failed at 14:41:08.613664 after 19.66 seconds
[0m14:41:08.614618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B4A81300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B1856810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000169B215E2D0>]}
[0m14:41:08.615324 [debug] [MainThread]: Flushing usage events
[0m14:41:09.095320 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:55:19.672704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9C506660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9D001F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9E6C7ED0>]}


============================== 14:55:19.683147 | 2b4ac826-0063-404f-a3dc-31e2c6c3a79a ==============================
[0m14:55:19.683147 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:55:19.684417 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'target_path': 'None', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'printer_width': '80', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m14:55:20.376641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2b4ac826-0063-404f-a3dc-31e2c6c3a79a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9C9FE190>]}
[0m14:55:20.532852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2b4ac826-0063-404f-a3dc-31e2c6c3a79a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9E5D2CF0>]}
[0m14:55:20.534603 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:55:21.022857 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:55:21.414009 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:55:21.414578 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:55:21.500013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2b4ac826-0063-404f-a3dc-31e2c6c3a79a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9ECE5050>]}
[0m14:55:21.655503 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:55:21.660242 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:55:21.713272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2b4ac826-0063-404f-a3dc-31e2c6c3a79a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9C4FE030>]}
[0m14:55:21.714010 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:55:21.714935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b4ac826-0063-404f-a3dc-31e2c6c3a79a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9EA273F0>]}
[0m14:55:21.719688 [info ] [MainThread]: 
[0m14:55:21.720552 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:55:21.721238 [info ] [MainThread]: 
[0m14:55:21.722161 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:55:21.736138 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:55:21.755205 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:28.598402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F0C06660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F1701F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F2DC7ED0>]}


============================== 14:55:28.607681 | bbce1e25-7069-479b-ba9d-5685bb205885 ==============================
[0m14:55:28.607681 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:55:28.609147 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'log_format': 'default', 'target_path': 'None', 'printer_width': '80', 'invocation_command': 'dbt run', 'warn_error': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m14:55:29.091127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bbce1e25-7069-479b-ba9d-5685bb205885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F10FE190>]}
[0m14:55:29.145175 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000025FA1566850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:55:29.202068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bbce1e25-7069-479b-ba9d-5685bb205885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F2CD2CF0>]}
[0m14:55:29.203621 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:55:29.632033 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:55:29.929542 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:55:29.930071 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:55:30.006001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bbce1e25-7069-479b-ba9d-5685bb205885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F343D050>]}
[0m14:55:30.161245 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:55:30.164886 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:55:30.201048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bbce1e25-7069-479b-ba9d-5685bb205885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F0BFE030>]}
[0m14:55:30.201774 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:55:30.202479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bbce1e25-7069-479b-ba9d-5685bb205885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F31273F0>]}
[0m14:55:30.206311 [info ] [MainThread]: 
[0m14:55:30.207155 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:55:30.207857 [info ] [MainThread]: 
[0m14:55:30.208842 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:55:30.222134 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:55:30.240826 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:35.533579 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:55:35.536694 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:35.537808 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:55:35.538698 [debug] [MainThread]: On list_: No close available on handle
[0m14:55:35.539791 [info ] [MainThread]: 
[0m14:55:35.544302 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 13.82 seconds (13.82s).
[0m14:55:35.546585 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x0000025FA15F95B0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:55:35.549306 [debug] [MainThread]: Command `dbt run` failed at 14:55:35.548900 after 16.18 seconds
[0m14:55:35.550253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FA15FD160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9E4D6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9EE28290>]}
[0m14:55:35.551033 [debug] [MainThread]: Flushing usage events
[0m14:55:36.053994 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:55:36.566675 [debug] [ThreadPool]: dbt_clickhouse adapter: Got a retryable error when attempting to open a clickhouse connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001B1F5C7A850>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:55:42.965749 [debug] [ThreadPool]: dbt_clickhouse adapter: Error running SQL: macro list_schemas
[0m14:55:42.969288 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:42.970369 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:55:42.972563 [debug] [MainThread]: On list_: No close available on handle
[0m14:55:42.973901 [info ] [MainThread]: 
[0m14:55:42.976013 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 12.77 seconds (12.77s).
[0m14:55:42.978833 [error] [MainThread]: Encountered an error:
Database Error
  Error HTTPConnectionPool(host='clickhouse', port=8123): Max retries exceeded with url: /? (Caused by NameResolutionError("<urllib3.connection.HTTPConnection object at 0x000001B1F5D0D5B0>: Failed to resolve 'clickhouse' ([Errno 11001] getaddrinfo failed)")) executing HTTP request attempt 1 (http://clickhouse:8123)
[0m14:55:42.984411 [debug] [MainThread]: Command `dbt run` failed at 14:55:42.983744 after 14.59 seconds
[0m14:55:42.986765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F5D11090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F2BD6810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1F3528290>]}
[0m14:55:42.988070 [debug] [MainThread]: Flushing usage events
[0m14:55:43.521334 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:03:59.031361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153C3CF6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153C47F1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153C5EB7ED0>]}


============================== 15:03:59.040152 | 72366955-245c-4c48-b44d-9685f1c4ccd9 ==============================
[0m15:03:59.040152 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:03:59.041130 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'write_json': 'True', 'use_colors': 'True', 'log_format': 'default', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt debug', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m15:03:59.096586 [info ] [MainThread]: dbt version: 1.10.13
[0m15:03:59.097819 [info ] [MainThread]: python version: 3.13.5
[0m15:03:59.098887 [info ] [MainThread]: python path: C:\Users\marti\anaconda3\python.exe
[0m15:03:59.099813 [info ] [MainThread]: os info: Windows-11-10.0.22621-SP0
[0m15:03:59.241539 [info ] [MainThread]: Using profiles dir at C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt
[0m15:03:59.242529 [info ] [MainThread]: Using profiles.yml file at C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\profiles.yml
[0m15:03:59.243212 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\dbt_project.yml
[0m15:03:59.244618 [info ] [MainThread]: adapter type: clickhouse
[0m15:03:59.245637 [info ] [MainThread]: adapter version: 1.9.5
[0m15:03:59.450593 [info ] [MainThread]: Configuration:
[0m15:03:59.451444 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:03:59.452041 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:03:59.452708 [info ] [MainThread]: Required dependencies:
[0m15:03:59.453382 [debug] [MainThread]: Executing "git --help"
[0m15:03:59.531816 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:03:59.532750 [debug] [MainThread]: STDERR: "b''"
[0m15:03:59.533390 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:03:59.534062 [info ] [MainThread]: Connection:
[0m15:03:59.534774 [info ] [MainThread]:   driver: None
[0m15:03:59.535445 [info ] [MainThread]:   host: localhost
[0m15:03:59.536121 [info ] [MainThread]:   port: 8123
[0m15:03:59.536942 [info ] [MainThread]:   user: default
[0m15:03:59.537664 [info ] [MainThread]:   schema: default
[0m15:03:59.538328 [info ] [MainThread]:   retries: 1
[0m15:03:59.538991 [info ] [MainThread]:   cluster: None
[0m15:03:59.539647 [info ] [MainThread]:   database_engine: None
[0m15:03:59.540307 [info ] [MainThread]:   cluster_mode: False
[0m15:03:59.540996 [info ] [MainThread]:   secure: False
[0m15:03:59.541646 [info ] [MainThread]:   verify: True
[0m15:03:59.542385 [info ] [MainThread]:   client_cert: None
[0m15:03:59.543108 [info ] [MainThread]:   client_cert_key: None
[0m15:03:59.543796 [info ] [MainThread]:   connect_timeout: 10
[0m15:03:59.544451 [info ] [MainThread]:   send_receive_timeout: 300
[0m15:03:59.545134 [info ] [MainThread]:   sync_request_timeout: 5
[0m15:03:59.545797 [info ] [MainThread]:   compress_block_size: 1048576
[0m15:03:59.546459 [info ] [MainThread]:   compression: 
[0m15:03:59.547116 [info ] [MainThread]:   check_exchange: True
[0m15:03:59.547777 [info ] [MainThread]:   custom_settings: None
[0m15:03:59.548418 [info ] [MainThread]:   use_lw_deletes: False
[0m15:03:59.549082 [info ] [MainThread]:   allow_automatic_deduplication: False
[0m15:03:59.549898 [info ] [MainThread]:   tcp_keepalive: False
[0m15:03:59.551071 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:03:59.985750 [debug] [MainThread]: Acquiring new clickhouse connection 'debug'
[0m15:03:59.986299 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:04:02.006538 [debug] [MainThread]: dbt_clickhouse adapter: On debug: select 1 as id...
[0m15:04:02.051739 [debug] [MainThread]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:04:02.114169 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:04:02.114973 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:04:02.116478 [debug] [MainThread]: Command `dbt debug` succeeded at 15:04:02.116277 after 3.39 seconds
[0m15:04:02.117041 [debug] [MainThread]: Connection 'debug' was left open.
[0m15:04:02.117608 [debug] [MainThread]: On debug: Close
[0m15:04:02.118279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153C8AFE8B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153C8D9D490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153C6021F20>]}
[0m15:04:02.118960 [debug] [MainThread]: Flushing usage events
[0m15:04:02.633232 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:04:16.479664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F990EC6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9919BDF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F993087ED0>]}


============================== 15:04:16.489919 | 495e4f26-727e-4c2e-8109-5e920ffb4f6f ==============================
[0m15:04:16.489919 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:04:16.490972 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'printer_width': '80', 'no_print': 'None', 'target_path': 'None', 'invocation_command': 'dbt run --full-refresh', 'warn_error': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m15:04:17.115885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9913AA190>]}
[0m15:04:17.280431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F992F92AD0>]}
[0m15:04:17.282197 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:04:17.801746 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:04:18.068815 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:04:18.069688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F993146050>]}
[0m15:04:21.304669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F993807890>]}
[0m15:04:21.555519 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:04:21.561872 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:04:21.640104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9937F5630>]}
[0m15:04:21.641447 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:04:21.646374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9939C76C0>]}
[0m15:04:21.654466 [info ] [MainThread]: 
[0m15:04:21.656661 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:04:21.658098 [info ] [MainThread]: 
[0m15:04:21.659557 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:04:21.687383 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:04:21.720205 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:04:23.533905 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:04:23.579537 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:04:23.616393 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:04:23.631452 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:04:24.169298 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:04:24.228576 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:04:24.238561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F993970710>]}
[0m15:04:24.258389 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:04:24.259424 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:04:24.260741 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:04:24.261413 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:04:24.273013 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:04:24.274729 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:04:24.301387 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:04:24.322318 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:04:24.326239 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:04:24.865677 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:04:24.916491 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:24.946607 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:04:24.948750 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.003390 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.042410 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9962C15A0>]}
[0m15:04:25.043593 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.78s]
[0m15:04:25.044874 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:04:25.045679 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:04:25.046958 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:04:25.048065 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:04:25.048675 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:04:25.051358 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:04:25.052710 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:04:25.056593 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:04:25.057529 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:04:25.058391 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:04:25.107504 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.110561 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:04:25.112051 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.163886 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.167051 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F996720F50>]}
[0m15:04:25.168013 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:04:25.169258 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:04:25.169916 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:04:25.171373 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:04:25.172650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:04:25.173253 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:04:25.176062 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:04:25.177756 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:04:25.182543 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:04:25.183357 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:04:25.188835 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:04:25.235690 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.238402 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:04:25.239919 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.290836 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.294032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9965FB3D0>]}
[0m15:04:25.295029 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:04:25.296349 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:04:25.296960 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:04:25.297705 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:04:25.298775 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:04:25.299369 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:04:25.301799 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:04:25.303459 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:04:25.308286 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:04:25.309460 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:04:25.310871 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:04:25.358810 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.362116 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:04:25.363781 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.418024 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.420877 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F996716820>]}
[0m15:04:25.421724 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.12s]
[0m15:04:25.422802 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:04:25.423382 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:04:25.424396 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:04:25.425601 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:04:25.426224 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:04:25.430689 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:04:25.432426 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:04:25.438216 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:04:25.439413 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:04:25.440607 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:04:25.487703 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.490340 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:04:25.491882 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.544438 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.551759 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9967636B0>]}
[0m15:04:25.553735 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.13s]
[0m15:04:25.556011 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:04:25.557473 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:04:25.559327 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:04:25.561157 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:04:25.562794 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:04:25.567999 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:04:25.569841 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:04:25.575134 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:04:25.576403 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:04:25.577959 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:04:25.627787 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.630813 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:04:25.632258 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.682237 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.686138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F996763530>]}
[0m15:04:25.687425 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.13s]
[0m15:04:25.688773 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:04:25.689477 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:04:25.690473 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:04:25.691868 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:04:25.692931 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:04:25.699517 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:04:25.701628 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:04:25.707570 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:04:25.708794 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:04:25.710022 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:04:25.762947 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.766623 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:04:25.768189 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:04:25.821329 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:25.824294 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9966D3230>]}
[0m15:04:25.825200 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.13s]
[0m15:04:25.826442 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:04:25.827064 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:04:25.828107 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:04:25.829471 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:04:25.830130 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:04:25.834247 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:04:25.835602 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:04:25.914225 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  StopPointId as stop_point_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:04:25.971457 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:04:26.003291 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:04:26.053057 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.061531 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:04:26.063570 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_point_id", "quay_name", "location", "parent_stop_point_id")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  StopPointId as stop_point_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:04:26.141394 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.08 seconds
[0m15:04:26.151694 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:04:26.160208 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:04:26.171273 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:04:26.217412 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.220769 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F996763D10>]}
[0m15:04:26.222138 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.39s]
[0m15:04:26.223590 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:04:26.224606 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:04:26.225998 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:04:26.227080 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:04:26.227796 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:04:26.231672 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:04:26.233241 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:04:26.240269 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:04:26.294799 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.299056 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:04:26.346794 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.349513 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:04:26.351142 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:04:26.407266 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.409155 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:04:26.416221 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:04:26.421350 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:04:26.464985 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:04:26.467066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9967636B0>]}
[0m15:04:26.468089 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.24s]
[0m15:04:26.469188 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:04:26.469763 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:04:26.470471 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:04:26.471527 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:04:26.472161 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:04:26.475953 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:04:26.477567 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:04:26.484740 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:04:26.539329 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.544913 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:04:26.595322 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.598471 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:04:26.600233 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:04:26.655949 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.657637 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:04:26.664489 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:04:26.671354 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:04:26.717018 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:04:26.719036 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9967C32F0>]}
[0m15:04:26.719953 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m15:04:26.721000 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:04:26.721665 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:04:26.722387 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:04:26.723477 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:04:26.724092 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:04:26.728120 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:04:26.729707 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:04:26.734966 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:04:26.788621 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.792267 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:04:26.838662 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:04:26.841018 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:04:26.842415 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:04:26.902120 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:04:26.904472 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:04:26.912119 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:04:26.921323 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:04:26.965008 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:04:26.967108 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F996763E90>]}
[0m15:04:26.968012 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.24s]
[0m15:04:26.969342 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:04:26.970065 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_point
[0m15:04:26.971177 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_point` ...................... [RUN]
[0m15:04:26.972216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_point)
[0m15:04:26.972782 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_point
[0m15:04:26.979823 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_point"
[0m15:04:26.982021 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_point
[0m15:04:26.987835 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  on s.StopPointId = sp.StopPointId
where sp.StopPointId is not null
          )
        
        ...
[0m15:04:27.041081 [debug] [Thread-1 (]: dbt_clickhouse adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  on s.StopPointId = sp.StopPointId
where sp.StopPointId is not null
          )
        
        
[0m15:04:27.124580 [debug] [Thread-1 (]: Database Error in model dim_stop_point (models\gold\dim_stop_point.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Identifier 'sp.StopPointId' cannot be resolved from table with name sp. In scope SELECT row_number() OVER (ORDER BY s.ServiceJourneyId ASC, s.QuayId ASC, s.AimedArrivalTime ASC) AS event_id, sp.StopPlaceId AS stop_place_id, CAST(s.AimedArrivalTime, 'Date') AS date_id, sp.Name AS stop_point_name, sp.ShortName AS stop_point_short_name, sp.Centroid_Long, sp.Centroid_Lat, sp.StopPlaceType, sp.ParentStopPlaceId, sp.Ingestion_Date FROM default.silver_stop_places AS sp LEFT JOIN default.silver_timetable AS s ON s.StopPointId = sp.StopPointId WHERE sp.StopPointId IS NOT NULL. Maybe you meant: ['sp.StopPlaceId']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:04:27.125470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '495e4f26-727e-4c2e-8109-5e920ffb4f6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F996763470>]}
[0m15:04:27.126473 [error] [Thread-1 (]: 12 of 12 ERROR creating sql table model `default`.`dim_stop_point` ............. [[31mERROR[0m in 0.15s]
[0m15:04:27.127768 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_point
[0m15:04:27.128786 [debug] [Thread-4 (]: Marking all children of 'model.dataengineering_dbt.dim_stop_point' to be skipped because of status 'error'.  Reason: Database Error in model dim_stop_point (models\gold\dim_stop_point.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Identifier 'sp.StopPointId' cannot be resolved from table with name sp. In scope SELECT row_number() OVER (ORDER BY s.ServiceJourneyId ASC, s.QuayId ASC, s.AimedArrivalTime ASC) AS event_id, sp.StopPlaceId AS stop_place_id, CAST(s.AimedArrivalTime, 'Date') AS date_id, sp.Name AS stop_point_name, sp.ShortName AS stop_point_short_name, sp.Centroid_Long, sp.Centroid_Lat, sp.StopPlaceType, sp.ParentStopPlaceId, sp.Ingestion_Date FROM default.silver_stop_places AS sp LEFT JOIN default.silver_timetable AS s ON s.StopPointId = sp.StopPointId WHERE sp.StopPointId IS NOT NULL. Maybe you meant: ['sp.StopPlaceId']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123).
[0m15:04:27.131906 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:04:27.132426 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:04:27.132857 [debug] [MainThread]: On list_: Close
[0m15:04:27.133245 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:04:27.133651 [debug] [MainThread]: On list__default: Close
[0m15:04:27.134028 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_point' was left open.
[0m15:04:27.134432 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_point: Close
[0m15:04:27.135317 [info ] [MainThread]: 
[0m15:04:27.136013 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 5.48 seconds (5.48s).
[0m15:04:27.139763 [debug] [MainThread]: Command end result
[0m15:04:27.184073 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:04:27.187823 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:04:27.201776 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:04:27.202351 [info ] [MainThread]: 
[0m15:04:27.203064 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:04:27.203814 [info ] [MainThread]: 
[0m15:04:27.204684 [error] [MainThread]: [31mFailure in model dim_stop_point (models\gold\dim_stop_point.sql)[0m
[0m15:04:27.205486 [error] [MainThread]:   Database Error in model dim_stop_point (models\gold\dim_stop_point.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Identifier 'sp.StopPointId' cannot be resolved from table with name sp. In scope SELECT row_number() OVER (ORDER BY s.ServiceJourneyId ASC, s.QuayId ASC, s.AimedArrivalTime ASC) AS event_id, sp.StopPlaceId AS stop_place_id, CAST(s.AimedArrivalTime, 'Date') AS date_id, sp.Name AS stop_point_name, sp.ShortName AS stop_point_short_name, sp.Centroid_Long, sp.Centroid_Lat, sp.StopPlaceType, sp.ParentStopPlaceId, sp.Ingestion_Date FROM default.silver_stop_places AS sp LEFT JOIN default.silver_timetable AS s ON s.StopPointId = sp.StopPointId WHERE sp.StopPointId IS NOT NULL. Maybe you meant: ['sp.StopPlaceId']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:04:27.206704 [info ] [MainThread]: 
[0m15:04:27.207514 [info ] [MainThread]:   compiled code at target\compiled\dataengineering_dbt\models\gold\dim_stop_point.sql
[0m15:04:27.208304 [info ] [MainThread]: 
[0m15:04:27.209066 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m15:04:27.210975 [debug] [MainThread]: Command `dbt run` failed at 15:04:27.210715 after 10.97 seconds
[0m15:04:27.211691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F99679F530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F991A11250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9967EF770>]}
[0m15:04:27.212315 [debug] [MainThread]: Flushing usage events
[0m15:04:27.846735 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:10:07.608125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BA9BD6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAA6CDF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BABD97ED0>]}


============================== 15:10:07.615374 | 45be4344-8565-4149-8d57-fed5b25b4f1c ==============================
[0m15:10:07.615374 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:10:07.616445 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'target_path': 'None', 'use_colors': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run --full-refresh', 'printer_width': '80', 'write_json': 'True'}
[0m15:10:08.244827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAA0CE190>]}
[0m15:10:08.376785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BABCA2AD0>]}
[0m15:10:08.378496 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:10:08.869840 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:10:09.219233 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:10:09.220655 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_stop_point.sql
[0m15:10:09.881456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAC3C9550>]}
[0m15:10:10.054889 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:10:10.059148 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:10:10.094293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BA9BCE030>]}
[0m15:10:10.095034 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:10:10.095780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAC03E970>]}
[0m15:10:10.099722 [info ] [MainThread]: 
[0m15:10:10.100584 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:10:10.101239 [info ] [MainThread]: 
[0m15:10:10.102119 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:10:10.115743 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:10:10.131243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:11.518404 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:10:11.566754 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:11.590875 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:10:11.599434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:12.117933 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:10:12.170029 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:12.176961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAEE33A00>]}
[0m15:10:12.181943 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:10:12.182713 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:10:12.183648 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:10:12.184200 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:10:12.193130 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:10:12.194641 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:10:12.214742 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:10:12.229405 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:10:12.232557 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:10:12.745867 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:10:12.791736 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:12.819629 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:10:12.821157 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:12.874401 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:12.912578 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF01D790>]}
[0m15:10:12.914549 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.73s]
[0m15:10:12.916733 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:10:12.918245 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:10:12.920300 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:10:12.921900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:10:12.923179 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:10:12.929951 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:10:12.932229 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:10:12.940418 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:10:12.942748 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:10:12.944968 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:10:12.994038 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:12.999354 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:10:13.002129 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:13.057159 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.060662 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAEDDD7B0>]}
[0m15:10:13.061715 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.14s]
[0m15:10:13.062849 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:10:13.063444 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:10:13.064188 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:10:13.065437 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:10:13.066106 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:10:13.068724 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:10:13.070425 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:10:13.077174 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:10:13.077998 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:10:13.079529 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:10:13.127412 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.130060 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:10:13.131395 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:13.185858 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.188641 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF11F2F0>]}
[0m15:10:13.189509 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:10:13.190602 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:10:13.191183 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:10:13.191996 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:10:13.193041 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:10:13.193618 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:10:13.195841 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:10:13.197222 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:10:13.200272 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:10:13.200980 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:10:13.201815 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:10:13.247392 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:10:13.250568 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:10:13.252161 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:13.306425 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.309390 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF164AD0>]}
[0m15:10:13.310351 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.12s]
[0m15:10:13.311380 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:10:13.311936 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:10:13.312639 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:10:13.313613 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:10:13.314368 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:10:13.317886 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:10:13.319170 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:10:13.322297 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:10:13.323052 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:10:13.323795 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:10:13.375884 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.379681 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:10:13.381392 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:13.431250 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.435216 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF2F0440>]}
[0m15:10:13.436439 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.12s]
[0m15:10:13.437628 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:10:13.438178 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:10:13.438812 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:10:13.439912 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:10:13.440593 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:10:13.444533 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:10:13.446020 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:10:13.449538 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:10:13.450348 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:10:13.451076 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:10:13.504944 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.508298 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:10:13.510200 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:13.563783 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.568206 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF2786B0>]}
[0m15:10:13.569215 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.13s]
[0m15:10:13.570408 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:10:13.571100 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:10:13.572371 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:10:13.573822 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:10:13.574791 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:10:13.578889 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:10:13.580390 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:10:13.584038 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:10:13.584927 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:10:13.585703 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:10:13.632365 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.635667 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:10:13.637347 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:10:13.691086 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.696131 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF0C5670>]}
[0m15:10:13.697344 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m15:10:13.699055 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:10:13.700365 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:10:13.702011 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:10:13.703378 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:10:13.704411 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:10:13.712269 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:10:13.714144 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:10:13.861011 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  StopPointId as stop_point_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:10:13.915344 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:13.954524 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:10:14.004645 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.010452 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:10:14.011975 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_point_id", "quay_name", "location", "parent_stop_point_id")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  StopPointId as stop_point_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:10:14.080972 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:10:14.088320 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:10:14.095476 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:10:14.106557 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:10:14.158539 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.162175 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF1DBA70>]}
[0m15:10:14.163766 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.46s]
[0m15:10:14.165581 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:10:14.166282 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:10:14.167194 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:10:14.168227 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:10:14.168860 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:10:14.173419 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:10:14.176691 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:10:14.184450 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:10:14.237914 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.241869 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:10:14.287778 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.290620 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:10:14.292458 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:10:14.349249 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:10:14.351944 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:10:14.359536 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:10:14.364556 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:10:14.410101 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:10:14.413142 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF278DD0>]}
[0m15:10:14.414362 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.24s]
[0m15:10:14.415599 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:10:14.416209 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:10:14.416887 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:10:14.417720 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:10:14.418399 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:10:14.422010 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:10:14.423559 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:10:14.430109 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:10:14.487113 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:10:14.490694 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:10:14.539655 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.542786 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:10:14.544445 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:10:14.601363 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:10:14.603907 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:10:14.611300 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:10:14.616009 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:10:14.661923 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.663887 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF279910>]}
[0m15:10:14.664716 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m15:10:14.665747 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:10:14.666334 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:10:14.666958 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:10:14.667783 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:10:14.668346 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:10:14.671601 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:10:14.673162 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:10:14.679884 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:10:14.733725 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.737031 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:10:14.783540 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.785936 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:10:14.787340 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:10:14.843385 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:10:14.845181 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:10:14.851584 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:10:14.857870 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:10:14.906107 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:10:14.908190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF27A090>]}
[0m15:10:14.908978 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.24s]
[0m15:10:14.910112 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:10:14.910640 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_point
[0m15:10:14.911270 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_point` ...................... [RUN]
[0m15:10:14.912286 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_point)
[0m15:10:14.913387 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_point
[0m15:10:14.918057 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_point"
[0m15:10:14.919553 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_point
[0m15:10:14.927753 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPointId is not null
          )
        
        ...
[0m15:10:14.979735 [debug] [Thread-1 (]: dbt_clickhouse adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPointId is not null
          )
        
        
[0m15:10:14.994232 [debug] [Thread-1 (]: Database Error in model dim_stop_point (models\gold\dim_stop_point.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Identifier 'sp.StopPointId' cannot be resolved from table with name sp. In scope SELECT row_number() OVER (ORDER BY s.ServiceJourneyId ASC, s.QuayId ASC, s.AimedArrivalTime ASC) AS event_id, sp.StopPlaceId AS stop_place_id, CAST(s.AimedArrivalTime, 'Date') AS date_id, sp.Name AS stop_point_name, sp.ShortName AS stop_point_short_name, sp.Centroid_Long, sp.Centroid_Lat, sp.StopPlaceType, sp.ParentStopPlaceId, sp.Ingestion_Date FROM default.silver_stop_places AS sp LEFT JOIN default.silver_timetable AS s ON s.StopPointId = sp.StopPlaceId WHERE sp.StopPointId IS NOT NULL. Maybe you meant: ['sp.StopPlaceId']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:10:14.994981 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45be4344-8565-4149-8d57-fed5b25b4f1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF27B0B0>]}
[0m15:10:14.995822 [error] [Thread-1 (]: 12 of 12 ERROR creating sql table model `default`.`dim_stop_point` ............. [[31mERROR[0m in 0.08s]
[0m15:10:14.996926 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_point
[0m15:10:14.997732 [debug] [Thread-4 (]: Marking all children of 'model.dataengineering_dbt.dim_stop_point' to be skipped because of status 'error'.  Reason: Database Error in model dim_stop_point (models\gold\dim_stop_point.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Identifier 'sp.StopPointId' cannot be resolved from table with name sp. In scope SELECT row_number() OVER (ORDER BY s.ServiceJourneyId ASC, s.QuayId ASC, s.AimedArrivalTime ASC) AS event_id, sp.StopPlaceId AS stop_place_id, CAST(s.AimedArrivalTime, 'Date') AS date_id, sp.Name AS stop_point_name, sp.ShortName AS stop_point_short_name, sp.Centroid_Long, sp.Centroid_Lat, sp.StopPlaceType, sp.ParentStopPlaceId, sp.Ingestion_Date FROM default.silver_stop_places AS sp LEFT JOIN default.silver_timetable AS s ON s.StopPointId = sp.StopPlaceId WHERE sp.StopPointId IS NOT NULL. Maybe you meant: ['sp.StopPlaceId']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123).
[0m15:10:15.000753 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:10:15.001467 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:10:15.002079 [debug] [MainThread]: On list_: Close
[0m15:10:15.002584 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:10:15.003151 [debug] [MainThread]: On list__default: Close
[0m15:10:15.003566 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_point' was left open.
[0m15:10:15.004012 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_point: Close
[0m15:10:15.004830 [info ] [MainThread]: 
[0m15:10:15.005597 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 4.90 seconds (4.90s).
[0m15:10:15.010887 [debug] [MainThread]: Command end result
[0m15:10:15.056534 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:10:15.059890 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:10:15.072100 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:10:15.072773 [info ] [MainThread]: 
[0m15:10:15.073483 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:10:15.074193 [info ] [MainThread]: 
[0m15:10:15.075031 [error] [MainThread]: [31mFailure in model dim_stop_point (models\gold\dim_stop_point.sql)[0m
[0m15:10:15.075855 [error] [MainThread]:   Database Error in model dim_stop_point (models\gold\dim_stop_point.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Identifier 'sp.StopPointId' cannot be resolved from table with name sp. In scope SELECT row_number() OVER (ORDER BY s.ServiceJourneyId ASC, s.QuayId ASC, s.AimedArrivalTime ASC) AS event_id, sp.StopPlaceId AS stop_place_id, CAST(s.AimedArrivalTime, 'Date') AS date_id, sp.Name AS stop_point_name, sp.ShortName AS stop_point_short_name, sp.Centroid_Long, sp.Centroid_Lat, sp.StopPlaceType, sp.ParentStopPlaceId, sp.Ingestion_Date FROM default.silver_stop_places AS sp LEFT JOIN default.silver_timetable AS s ON s.StopPointId = sp.StopPlaceId WHERE sp.StopPointId IS NOT NULL. Maybe you meant: ['sp.StopPlaceId']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:10:15.076763 [info ] [MainThread]: 
[0m15:10:15.077459 [info ] [MainThread]:   compiled code at target\compiled\dataengineering_dbt\models\gold\dim_stop_point.sql
[0m15:10:15.078086 [info ] [MainThread]: 
[0m15:10:15.078756 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m15:10:15.080229 [debug] [MainThread]: Command `dbt run` failed at 15:10:15.080069 after 7.75 seconds
[0m15:10:15.080732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BA9B204D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF279E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAF279370>]}
[0m15:10:15.081234 [debug] [MainThread]: Flushing usage events
[0m15:10:15.705739 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:11:15.925930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E4EE6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E59E1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E70A7ED0>]}


============================== 15:11:15.933917 | 234cf263-a361-4c68-9bca-5a433d1f1b7a ==============================
[0m15:11:15.933917 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:11:15.935297 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'use_colors': 'True', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'log_format': 'default', 'printer_width': '80', 'warn_error': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --full-refresh', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m15:11:16.524703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E53DE190>]}
[0m15:11:16.680427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E6FB2AD0>]}
[0m15:11:16.682935 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:11:17.225748 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:11:17.576624 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:11:17.577658 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_stop_point.sql
[0m15:11:18.288796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E76C9550>]}
[0m15:11:18.490902 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:11:18.495587 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:11:18.538797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E4EDE030>]}
[0m15:11:18.539680 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:11:18.540622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E7322970>]}
[0m15:11:18.544831 [info ] [MainThread]: 
[0m15:11:18.546095 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:11:18.546904 [info ] [MainThread]: 
[0m15:11:18.547965 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:11:18.563740 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:11:18.585895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:11:20.141850 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:11:20.190451 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:20.224586 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:11:20.234225 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:11:20.761537 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:11:20.809207 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:20.816795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA133AD0>]}
[0m15:11:20.822271 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:11:20.823167 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:11:20.824239 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:11:20.824832 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:11:20.834810 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:11:20.836424 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:11:20.865418 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:11:20.882482 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:11:20.886035 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:11:21.405729 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:11:21.460131 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.488885 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:11:21.490677 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:21.543121 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.582758 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA31D790>]}
[0m15:11:21.583895 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.76s]
[0m15:11:21.585245 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:11:21.585949 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:11:21.587214 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:11:21.588346 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:11:21.589007 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:11:21.592356 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:11:21.594013 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:11:21.598345 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:11:21.599448 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:11:21.600341 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:11:21.647503 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.650839 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:11:21.652433 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:21.708235 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.711611 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA0DD7B0>]}
[0m15:11:21.712614 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:11:21.713923 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:11:21.714591 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:11:21.715717 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:11:21.716899 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:11:21.717551 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:11:21.720235 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:11:21.721829 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:11:21.726497 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:11:21.727380 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:11:21.728786 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:11:21.775455 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.778489 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:11:21.780045 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:21.834736 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.837929 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA41F2F0>]}
[0m15:11:21.839166 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:11:21.840422 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:11:21.841232 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:11:21.842196 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:11:21.843303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:11:21.843981 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:11:21.846569 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:11:21.848094 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:11:21.853048 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:11:21.854022 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:11:21.854850 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:11:21.903217 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.907764 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:11:21.909464 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:21.958822 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:21.961851 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E2EA1050>]}
[0m15:11:21.962775 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.12s]
[0m15:11:21.963955 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:11:21.964735 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:11:21.965762 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:11:21.966651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:11:21.967245 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:11:21.971246 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:11:21.972929 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:11:21.977474 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:11:21.978317 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:11:21.979133 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:11:22.031454 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.034543 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:11:22.036377 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:22.087637 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.090722 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA5E83D0>]}
[0m15:11:22.091655 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.12s]
[0m15:11:22.092865 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:11:22.093489 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:11:22.094247 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:11:22.095427 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:11:22.096148 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:11:22.100018 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:11:22.101591 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:11:22.105235 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:11:22.106224 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:11:22.107528 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:11:22.156094 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.159250 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:11:22.160762 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:22.210551 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.213597 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA57D490>]}
[0m15:11:22.214518 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m15:11:22.215693 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:11:22.216306 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:11:22.217168 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:11:22.218139 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:11:22.218916 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:11:22.222764 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:11:22.224458 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:11:22.228132 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:11:22.229294 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:11:22.230559 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:11:22.279283 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.284706 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:11:22.286769 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:11:22.338535 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.341763 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA3C5130>]}
[0m15:11:22.342696 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m15:11:22.343898 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:11:22.344526 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:11:22.345290 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:11:22.346520 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:11:22.347166 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:11:22.351601 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:11:22.353081 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:11:22.434722 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  StopPointId as stop_point_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:11:22.491253 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:22.518358 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:11:22.567324 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.573415 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:11:22.575406 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_point_id", "quay_name", "location", "parent_stop_point_id")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  StopPointId as stop_point_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:11:22.641985 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:11:22.650056 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:11:22.658223 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:11:22.668481 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:11:22.713497 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:11:22.716035 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA4DF890>]}
[0m15:11:22.717189 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.37s]
[0m15:11:22.718487 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:11:22.719153 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:11:22.720083 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:11:22.721086 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:11:22.721731 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:11:22.726413 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:11:22.727923 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:11:22.734016 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:11:22.792669 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:22.797086 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:11:22.847927 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:22.850933 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:11:22.852460 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:11:22.908571 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:22.910679 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:11:22.918383 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:11:22.924320 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:11:22.969732 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:11:22.972053 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA553B30>]}
[0m15:11:22.973028 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.25s]
[0m15:11:22.974223 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:11:22.974858 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:11:22.975747 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:11:22.976733 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:11:22.977347 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:11:22.981448 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:11:22.983043 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:11:22.991444 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:11:23.048123 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:23.053382 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:11:23.103482 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:23.107150 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:11:23.109248 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:11:23.173876 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:23.176884 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:11:23.185515 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:11:23.193691 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:11:23.237681 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:11:23.241065 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA57F6B0>]}
[0m15:11:23.242470 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.26s]
[0m15:11:23.244099 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:11:23.244787 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:11:23.245709 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:11:23.246702 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:11:23.247305 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:11:23.251248 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:11:23.252800 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:11:23.259263 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:11:23.315254 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:23.321345 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:11:23.371108 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:23.373872 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:11:23.375499 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:11:23.436398 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:23.438500 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:11:23.445703 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:11:23.451565 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:11:23.497464 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:23.499886 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA57F650>]}
[0m15:11:23.500805 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.25s]
[0m15:11:23.502016 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:11:23.502651 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_point
[0m15:11:23.503647 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_point` ...................... [RUN]
[0m15:11:23.504824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_point)
[0m15:11:23.505492 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_point
[0m15:11:23.511193 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_point"
[0m15:11:23.512841 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_point
[0m15:11:23.519039 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:11:23.576103 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:11:23.580760 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

    select name, type from system.columns where table = 'dim_stop_point__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:11:23.627158 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:23.630958 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_point"
[0m15:11:23.633409 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_point__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:11:23.719465 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.08 seconds
[0m15:11:23.722013 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
EXCHANGE TABLES `default`.`dim_stop_point__dbt_backup` AND `default`.`dim_stop_point` 
  
  ...
[0m15:11:23.730537 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:11:23.743795 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
drop table if exists `default`.`dim_stop_point__dbt_backup` 
  ...
[0m15:11:23.790163 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:11:23.793625 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '234cf263-a361-4c68-9bca-5a433d1f1b7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA4DFA10>]}
[0m15:11:23.795333 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_point` ................. [[32mOK[0m in 0.29s]
[0m15:11:23.797913 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_point
[0m15:11:23.800979 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:11:23.802000 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:11:23.802815 [debug] [MainThread]: On list_: Close
[0m15:11:23.803503 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:11:23.804203 [debug] [MainThread]: On list__default: Close
[0m15:11:23.804852 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_point' was left open.
[0m15:11:23.805556 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_point: Close
[0m15:11:23.807330 [info ] [MainThread]: 
[0m15:11:23.808473 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 5.26 seconds (5.26s).
[0m15:11:23.820678 [debug] [MainThread]: Command end result
[0m15:11:23.905149 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:11:23.909905 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:11:23.931321 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:11:23.932066 [info ] [MainThread]: 
[0m15:11:23.933007 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:11:23.933977 [info ] [MainThread]: 
[0m15:11:23.934944 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m15:11:23.937196 [debug] [MainThread]: Command `dbt run` succeeded at 15:11:23.936935 after 8.27 seconds
[0m15:11:23.937902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4E6F28FB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA4DC530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4EA4DC650>]}
[0m15:11:23.938770 [debug] [MainThread]: Flushing usage events
[0m15:11:24.547636 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:16:50.654564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335CEE6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335D9DDF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335F0A7ED0>]}


============================== 15:16:50.666729 | 35d2b4b6-0a86-4c00-845e-3a9abe681d33 ==============================
[0m15:16:50.666729 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:16:50.669123 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'invocation_command': 'dbt run --full-refresh', 'no_print': 'None', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'target_path': 'None'}
[0m15:16:51.312500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335D3DE190>]}
[0m15:16:51.464407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335EFB2AD0>]}
[0m15:16:51.467244 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:16:52.022853 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:16:52.490682 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:16:52.491714 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\staging\stg_netex_quays.sql
[0m15:16:52.492474 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_quay.sql
[0m15:16:53.428411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335F685D50>]}
[0m15:16:53.580934 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:16:53.584907 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:16:53.632768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335CEDE030>]}
[0m15:16:53.633513 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:16:53.634299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335F390D70>]}
[0m15:16:53.638420 [info ] [MainThread]: 
[0m15:16:53.639330 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:16:53.639998 [info ] [MainThread]: 
[0m15:16:53.640937 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:16:53.658773 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:16:53.674604 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:16:55.268269 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:16:55.317033 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:55.353904 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:16:55.364090 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:16:55.887626 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:16:55.935015 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:55.942381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335F7F53D0>]}
[0m15:16:55.947756 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:16:55.948604 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:16:55.949695 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:16:55.950302 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:16:55.960506 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:16:55.962096 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:16:55.984007 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:16:56.004110 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:16:56.007610 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:16:56.551913 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:16:56.598017 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:56.630043 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:16:56.632233 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:56.684637 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:56.722273 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023362249910>]}
[0m15:16:56.723378 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.77s]
[0m15:16:56.724598 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:16:56.725240 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:16:56.726386 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:16:56.727528 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:16:56.728141 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:16:56.730724 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:16:56.732167 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:16:56.735950 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:16:56.736792 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:16:56.737577 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:16:56.789579 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:56.792517 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:16:56.794326 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:56.844665 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:56.849157 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002336221A2B0>]}
[0m15:16:56.850440 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:16:56.851646 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:16:56.852276 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:16:56.853359 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:16:56.854564 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:16:56.855187 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:16:56.857809 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:16:56.859908 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:16:56.863522 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:16:56.864291 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:16:56.865588 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:16:56.913320 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:56.916115 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:16:56.917708 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:56.968693 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:56.971823 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335F2EEE90>]}
[0m15:16:56.972792 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:16:56.974207 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:16:56.975124 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:16:56.976297 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:16:56.977334 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:16:56.977947 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:16:56.980700 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:16:56.982448 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:16:56.985419 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:16:56.986466 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:16:56.987590 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:16:57.037555 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.040930 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:16:57.042532 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:57.092541 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.095528 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233625C6550>]}
[0m15:16:57.096399 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.12s]
[0m15:16:57.097582 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:16:57.098183 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:16:57.099196 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:16:57.100315 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:16:57.100926 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:16:57.105441 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:16:57.107671 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:16:57.113515 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:16:57.114354 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:16:57.115113 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:16:57.161267 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.164050 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:16:57.165646 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:57.216885 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.220050 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233627246E0>]}
[0m15:16:57.221010 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.12s]
[0m15:16:57.222120 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:16:57.222716 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:16:57.223826 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:16:57.224959 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:16:57.225664 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:16:57.229231 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:16:57.230641 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:16:57.233737 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:16:57.234671 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:16:57.235581 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:16:57.285992 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.288812 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:16:57.290228 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:57.340447 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.343329 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002336271C830>]}
[0m15:16:57.344321 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m15:16:57.345801 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:16:57.346709 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:16:57.348101 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:16:57.349105 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:16:57.349693 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:16:57.353168 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:16:57.354624 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:16:57.359079 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:16:57.360317 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:16:57.361351 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:16:57.410435 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.414529 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:16:57.416394 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:16:57.467724 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.470719 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002336271CA10>]}
[0m15:16:57.472136 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m15:16:57.473267 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:16:57.473865 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:16:57.474573 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:16:57.475731 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:16:57.476353 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:16:57.480336 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:16:57.481752 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:16:57.557358 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:16:57.605542 [debug] [Thread-1 (]: dbt_clickhouse adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location,
  ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        
[0m15:16:57.621558 [debug] [Thread-1 (]: Database Error in model dim_quay (models\gold\dim_quay.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Unknown expression identifier `ParentStopPointId` in scope SELECT DISTINCT QuayId AS quay_id, StopPlaceId AS stop_place_id, QuayName AS quay_name, Location AS location, ParentStopPointId AS parent_stop_point_id FROM default.stg_netex_quays AS q LEFT JOIN default.silver_timetable AS s ON q.QuayId = s.QuayId WHERE q.QuayId IS NOT NULL. Maybe you meant: ['parent_stop_point_id']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:16:57.622415 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233626B7410>]}
[0m15:16:57.623341 [error] [Thread-1 (]: 8 of 12 ERROR creating sql table model `default`.`dim_quay` .................... [[31mERROR[0m in 0.15s]
[0m15:16:57.624534 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:16:57.625142 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:16:57.625976 [debug] [Thread-4 (]: Marking all children of 'model.dataengineering_dbt.dim_quay' to be skipped because of status 'error'.  Reason: Database Error in model dim_quay (models\gold\dim_quay.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Unknown expression identifier `ParentStopPointId` in scope SELECT DISTINCT QuayId AS quay_id, StopPlaceId AS stop_place_id, QuayName AS quay_name, Location AS location, ParentStopPointId AS parent_stop_point_id FROM default.stg_netex_quays AS q LEFT JOIN default.silver_timetable AS s ON q.QuayId = s.QuayId WHERE q.QuayId IS NOT NULL. Maybe you meant: ['parent_stop_point_id']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123).
[0m15:16:57.626999 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:16:57.629079 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:16:57.629674 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:16:57.633318 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:16:57.634787 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:16:57.642781 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:16:57.697197 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.721264 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:16:57.769246 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.776044 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:16:57.777931 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:16:57.833466 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.840155 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:16:57.846723 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:16:57.855591 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:16:57.899633 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:16:57.901702 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233626B6B70>]}
[0m15:16:57.902609 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.27s]
[0m15:16:57.903700 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:16:57.904378 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:16:57.905236 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:16:57.906566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:16:57.907445 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:16:57.913693 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:16:57.915449 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:16:57.922804 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:16:57.976152 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:57.982724 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:16:58.029435 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.033452 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:16:58.035222 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:16:58.089247 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.091081 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:16:58.097973 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:16:58.103781 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:16:58.147610 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:16:58.149740 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233627D3290>]}
[0m15:16:58.150623 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.24s]
[0m15:16:58.151781 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:16:58.152383 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:16:58.153318 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:16:58.154523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:16:58.155100 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:16:58.159346 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:16:58.161033 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:16:58.167865 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:16:58.219428 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.222830 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:16:58.269130 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.271536 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:16:58.272856 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:16:58.328137 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.329895 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:16:58.336434 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:16:58.343152 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:16:58.387570 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:16:58.389819 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233627D2C90>]}
[0m15:16:58.390655 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.24s]
[0m15:16:58.391746 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:16:58.392304 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_point
[0m15:16:58.392965 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_point` ...................... [RUN]
[0m15:16:58.393879 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_point)
[0m15:16:58.394571 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_point
[0m15:16:58.398940 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_point"
[0m15:16:58.400459 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_point
[0m15:16:58.405629 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:16:58.460837 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.464214 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

    select name, type from system.columns where table = 'dim_stop_point__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:16:58.514519 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.519271 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_point"
[0m15:16:58.521480 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_point__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:16:58.600510 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.08 seconds
[0m15:16:58.602742 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
EXCHANGE TABLES `default`.`dim_stop_point__dbt_backup` AND `default`.`dim_stop_point` 
  
  ...
[0m15:16:58.610926 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:16:58.617003 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
drop table if exists `default`.`dim_stop_point__dbt_backup` 
  ...
[0m15:16:58.663624 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:16:58.665840 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35d2b4b6-0a86-4c00-845e-3a9abe681d33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233627F3CB0>]}
[0m15:16:58.666728 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_point` ................. [[32mOK[0m in 0.27s]
[0m15:16:58.667870 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_point
[0m15:16:58.670044 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:16:58.670679 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:16:58.671148 [debug] [MainThread]: On list_: Close
[0m15:16:58.671568 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:16:58.672128 [debug] [MainThread]: On list__default: Close
[0m15:16:58.672804 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_point' was left open.
[0m15:16:58.673598 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_point: Close
[0m15:16:58.674681 [info ] [MainThread]: 
[0m15:16:58.675894 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 5.03 seconds (5.03s).
[0m15:16:58.680150 [debug] [MainThread]: Command end result
[0m15:16:58.728082 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:16:58.732148 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:16:58.745361 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:16:58.745946 [info ] [MainThread]: 
[0m15:16:58.746702 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:16:58.747464 [info ] [MainThread]: 
[0m15:16:58.748346 [error] [MainThread]: [31mFailure in model dim_quay (models\gold\dim_quay.sql)[0m
[0m15:16:58.749285 [error] [MainThread]:   Database Error in model dim_quay (models\gold\dim_quay.sql)
  Received ClickHouse exception, code: 47, server response: Code: 47. DB::Exception: Unknown expression identifier `ParentStopPointId` in scope SELECT DISTINCT QuayId AS quay_id, StopPlaceId AS stop_place_id, QuayName AS quay_name, Location AS location, ParentStopPointId AS parent_stop_point_id FROM default.stg_netex_quays AS q LEFT JOIN default.silver_timetable AS s ON q.QuayId = s.QuayId WHERE q.QuayId IS NOT NULL. Maybe you meant: ['parent_stop_point_id']. (UNKNOWN_IDENTIFIER) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:16:58.750694 [info ] [MainThread]: 
[0m15:16:58.751497 [info ] [MainThread]:   compiled code at target\compiled\dataengineering_dbt\models\gold\dim_quay.sql
[0m15:16:58.752191 [info ] [MainThread]: 
[0m15:16:58.752914 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m15:16:58.754822 [debug] [MainThread]: Command `dbt run` failed at 15:16:58.754622 after 8.53 seconds
[0m15:16:58.755452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002335D241730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233626B7E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233626B72F0>]}
[0m15:16:58.756039 [debug] [MainThread]: Flushing usage events
[0m15:16:59.350771 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:18:10.422780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF6F6A6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF7019DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF71867ED0>]}


============================== 15:18:10.429403 | f53445ff-40ac-4a2b-a686-64578a4625b2 ==============================
[0m15:18:10.429403 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:18:10.430423 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'invocation_command': 'dbt run --full-refresh', 'no_print': 'None', 'target_path': 'None', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m15:18:10.958130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF6FB9E190>]}
[0m15:18:11.092716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF71772AD0>]}
[0m15:18:11.094745 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:18:11.541633 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:18:11.850926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:18:11.851903 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_quay.sql
[0m15:18:12.713965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF71E12450>]}
[0m15:18:12.862038 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:18:12.865674 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:18:12.902429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF6F69E030>]}
[0m15:18:12.903155 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:18:12.903932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF71B31D30>]}
[0m15:18:12.907715 [info ] [MainThread]: 
[0m15:18:12.908573 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:18:12.909242 [info ] [MainThread]: 
[0m15:18:12.910130 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:18:12.931934 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:18:12.960346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:14.542683 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:18:14.595293 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:14.622732 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:18:14.632045 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:15.142450 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:18:15.191948 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:15.202261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF71F03C70>]}
[0m15:18:15.207768 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:18:15.208725 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:18:15.209749 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:18:15.210317 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:18:15.221604 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:18:15.224669 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:18:15.256966 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:18:15.283632 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:18:15.287533 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:18:15.806344 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:18:15.851928 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:18:15.876008 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:18:15.877724 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:15.926992 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:15.968438 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74A61910>]}
[0m15:18:15.969757 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.76s]
[0m15:18:15.970905 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:18:15.971543 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:18:15.972288 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:18:15.973639 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:18:15.974295 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:18:15.976739 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:18:15.978070 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:18:15.981593 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:18:15.982342 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:18:15.983058 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:18:16.036047 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.039061 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:18:16.040639 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:16.091383 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.094249 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF749DA2B0>]}
[0m15:18:16.095093 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:18:16.096180 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:18:16.096848 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:18:16.097524 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:18:16.098710 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:18:16.099427 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:18:16.101917 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:18:16.103251 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:18:16.107703 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:18:16.108482 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:18:16.109806 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:18:16.156060 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.158967 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:18:16.160639 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:16.211281 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.214049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF71F67C50>]}
[0m15:18:16.214888 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:18:16.215955 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:18:16.216557 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:16.217205 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:18:16.218011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:18:16.218684 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:16.221188 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:18:16.223228 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:16.227536 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:18:16.228289 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:18:16.229188 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:18:16.275872 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.279396 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:18:16.280863 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:16.331380 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.334123 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74D8AA50>]}
[0m15:18:16.335001 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.12s]
[0m15:18:16.336045 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:16.336649 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:18:16.337343 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:18:16.338429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:18:16.339019 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:18:16.344777 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:18:16.346509 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:18:16.349723 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:18:16.350933 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:18:16.351997 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:18:16.400584 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.403463 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:18:16.404918 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:16.455917 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.458713 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74EC0280>]}
[0m15:18:16.459554 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.12s]
[0m15:18:16.460671 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:18:16.461295 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:18:16.462460 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:18:16.463464 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:18:16.464062 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:18:16.467504 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:18:16.468808 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:18:16.472602 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:18:16.473767 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:18:16.474694 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:18:16.527972 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.530560 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:18:16.531963 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:16.582798 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.585676 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74E77350>]}
[0m15:18:16.586728 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m15:18:16.587911 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:18:16.588654 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:18:16.589718 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:18:16.590770 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:18:16.591347 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:18:16.594955 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:18:16.596351 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:18:16.599809 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:18:16.600595 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:18:16.601331 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:18:16.648636 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.651423 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:18:16.653191 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:16.703284 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:16.706857 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74E77950>]}
[0m15:18:16.707844 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m15:18:16.709025 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:18:16.709620 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:18:16.710329 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:18:16.711410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:18:16.712081 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:18:16.717141 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:18:16.718696 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:18:16.797123 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location,
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:18:16.845883 [debug] [Thread-1 (]: dbt_clickhouse adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location,
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        
[0m15:18:16.862492 [debug] [Thread-1 (]: Database Error in model dim_quay (models\gold\dim_quay.sql)
  Received ClickHouse exception, code: 62, server response: Code: 62. DB::Exception: Syntax error: failed at position 829 (`default`) (line 32, col 6): `default`.`stg_netex_quays` q
  left join `default`.`silver_timetable` s
    on q.QuayId = s.QuayId
  where q.QuayId is not null
            )
          
          . Expected one of: token sequence, Dot, token, OR, AND, IS NOT DISTINCT FROM, IS NULL, IS NOT NULL, BETWEEN, NOT BETWEEN, LIKE, ILIKE, NOT LIKE, NOT ILIKE, REGEXP, IN, NOT IN, GLOBAL IN, GLOBAL NOT IN, MOD, DIV, alias, AS, Comma, FROM, PREWHERE, WHERE, GROUP BY, WITH, HAVING, WINDOW, QUALIFY, ORDER BY, LIMIT, OFFSET, FETCH, SETTINGS, UNION, EXCEPT, INTERSECT. (SYNTAX_ERROR) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:18:16.863363 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74E77170>]}
[0m15:18:16.864308 [error] [Thread-1 (]: 8 of 12 ERROR creating sql table model `default`.`dim_quay` .................... [[31mERROR[0m in 0.15s]
[0m15:18:16.865538 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:18:16.866151 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:18:16.867004 [debug] [Thread-4 (]: Marking all children of 'model.dataengineering_dbt.dim_quay' to be skipped because of status 'error'.  Reason: Database Error in model dim_quay (models\gold\dim_quay.sql)
  Received ClickHouse exception, code: 62, server response: Code: 62. DB::Exception: Syntax error: failed at position 829 (`default`) (line 32, col 6): `default`.`stg_netex_quays` q
  left join `default`.`silver_timetable` s
    on q.QuayId = s.QuayId
  where q.QuayId is not null
            )
          
          . Expected one of: token sequence, Dot, token, OR, AND, IS NOT DISTINCT FROM, IS NULL, IS NOT NULL, BETWEEN, NOT BETWEEN, LIKE, ILIKE, NOT LIKE, NOT ILIKE, REGEXP, IN, NOT IN, GLOBAL IN, GLOBAL NOT IN, MOD, DIV, alias, AS, Comma, FROM, PREWHERE, WHERE, GROUP BY, WITH, HAVING, WINDOW, QUALIFY, ORDER BY, LIMIT, OFFSET, FETCH, SETTINGS, UNION, EXCEPT, INTERSECT. (SYNTAX_ERROR) (version 25.9.3.48 (official build)) (for url http://localhost:8123).
[0m15:18:16.868065 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:18:16.870090 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:18:16.870745 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:18:16.875384 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:18:16.876845 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:18:16.884166 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:18:16.940302 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:16.972482 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:17.020670 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.026295 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:18:17.027833 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:18:17.087930 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:17.095113 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:18:17.102504 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:17.111871 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:18:17.158562 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.161807 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74E76450>]}
[0m15:18:17.163071 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.29s]
[0m15:18:17.164378 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:18:17.165284 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:18:17.166325 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:18:17.167338 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:18:17.167902 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:18:17.173727 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:18:17.175266 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:18:17.180133 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:18:17.235171 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.244627 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:17.292062 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.295238 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:18:17.296646 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:18:17.359518 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:17.362809 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:18:17.370941 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:17.376854 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:18:17.426769 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.430296 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74F9A7B0>]}
[0m15:18:17.431735 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.26s]
[0m15:18:17.433535 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:18:17.434515 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:18:17.435618 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:18:17.436846 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:18:17.438142 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:18:17.443210 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:18:17.444993 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:18:17.452613 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:18:17.511036 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:17.514871 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:17.564313 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.566979 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:18:17.568599 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:18:17.623562 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.625463 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:18:17.632288 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:17.638038 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:18:17.682301 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:18:17.684439 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74F9A4B0>]}
[0m15:18:17.685329 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.25s]
[0m15:18:17.686467 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:18:17.687069 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_point
[0m15:18:17.687776 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_point` ...................... [RUN]
[0m15:18:17.689037 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_point)
[0m15:18:17.689761 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_point
[0m15:18:17.694342 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_point"
[0m15:18:17.695663 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_point
[0m15:18:17.701025 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:18:17.756003 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.759677 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

    select name, type from system.columns where table = 'dim_stop_point__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:17.808250 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:17.811312 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_point"
[0m15:18:17.813227 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_point__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:18:17.884260 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:18:17.886079 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
EXCHANGE TABLES `default`.`dim_stop_point__dbt_backup` AND `default`.`dim_stop_point` 
  
  ...
[0m15:18:17.892851 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:17.897960 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
drop table if exists `default`.`dim_stop_point__dbt_backup` 
  ...
[0m15:18:17.942665 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:18:17.945681 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f53445ff-40ac-4a2b-a686-64578a4625b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74EFC050>]}
[0m15:18:17.946879 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_point` ................. [[32mOK[0m in 0.26s]
[0m15:18:17.948119 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_point
[0m15:18:17.950106 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:18:17.950742 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:18:17.951192 [debug] [MainThread]: On list_: Close
[0m15:18:17.951619 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:18:17.952168 [debug] [MainThread]: On list__default: Close
[0m15:18:17.952577 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_point' was left open.
[0m15:18:17.952985 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_point: Close
[0m15:18:17.953876 [info ] [MainThread]: 
[0m15:18:17.954914 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 5.04 seconds (5.04s).
[0m15:18:17.959062 [debug] [MainThread]: Command end result
[0m15:18:18.010442 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:18:18.014489 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:18:18.028762 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:18:18.029498 [info ] [MainThread]: 
[0m15:18:18.030447 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:18:18.031152 [info ] [MainThread]: 
[0m15:18:18.031960 [error] [MainThread]: [31mFailure in model dim_quay (models\gold\dim_quay.sql)[0m
[0m15:18:18.032851 [error] [MainThread]:   Database Error in model dim_quay (models\gold\dim_quay.sql)
  Received ClickHouse exception, code: 62, server response: Code: 62. DB::Exception: Syntax error: failed at position 829 (`default`) (line 32, col 6): `default`.`stg_netex_quays` q
  left join `default`.`silver_timetable` s
    on q.QuayId = s.QuayId
  where q.QuayId is not null
            )
          
          . Expected one of: token sequence, Dot, token, OR, AND, IS NOT DISTINCT FROM, IS NULL, IS NOT NULL, BETWEEN, NOT BETWEEN, LIKE, ILIKE, NOT LIKE, NOT ILIKE, REGEXP, IN, NOT IN, GLOBAL IN, GLOBAL NOT IN, MOD, DIV, alias, AS, Comma, FROM, PREWHERE, WHERE, GROUP BY, WITH, HAVING, WINDOW, QUALIFY, ORDER BY, LIMIT, OFFSET, FETCH, SETTINGS, UNION, EXCEPT, INTERSECT. (SYNTAX_ERROR) (version 25.9.3.48 (official build)) (for url http://localhost:8123)
[0m15:18:18.033957 [info ] [MainThread]: 
[0m15:18:18.034744 [info ] [MainThread]:   compiled code at target\compiled\dataengineering_dbt\models\gold\dim_quay.sql
[0m15:18:18.035431 [info ] [MainThread]: 
[0m15:18:18.036217 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m15:18:18.038663 [debug] [MainThread]: Command `dbt run` failed at 15:18:18.038345 after 7.85 seconds
[0m15:18:18.039318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF6D738A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74FBF9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74FBFFB0>]}
[0m15:18:18.039901 [debug] [MainThread]: Flushing usage events
[0m15:18:18.656274 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:18:37.769526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176CA66660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176D561F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176EC27ED0>]}


============================== 15:18:37.782352 | 46c578be-ec2d-4449-a806-9538392b89e7 ==============================
[0m15:18:37.782352 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:18:37.785967 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'printer_width': '80', 'log_format': 'default', 'write_json': 'True', 'no_print': 'None', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run --full-refresh', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m15:18:38.502101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176CF5E190>]}
[0m15:18:38.630506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176EB32AD0>]}
[0m15:18:38.632742 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:18:39.167308 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:18:39.542669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:18:39.543746 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_quay.sql
[0m15:18:40.506597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176F27E450>]}
[0m15:18:40.672590 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:18:40.676762 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:18:40.720565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176CA5E030>]}
[0m15:18:40.721356 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:18:40.722302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176EF7DD30>]}
[0m15:18:40.726382 [info ] [MainThread]: 
[0m15:18:40.727269 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:18:40.727983 [info ] [MainThread]: 
[0m15:18:40.728952 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:18:40.748347 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:18:40.768963 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:42.364855 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:18:42.409732 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:18:42.436327 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:18:42.445511 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:42.964839 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:18:43.011882 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:43.020016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176F3EBC70>]}
[0m15:18:43.025242 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:18:43.026070 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:18:43.027095 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:18:43.027658 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:18:43.037400 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:18:43.039057 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:18:43.060800 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:18:43.081902 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:18:43.085727 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:18:43.609078 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:18:43.658523 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:43.687825 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:18:43.689617 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:43.742013 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:43.779049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021771E1D910>]}
[0m15:18:43.780451 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.75s]
[0m15:18:43.781710 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:18:43.782380 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:18:43.783587 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:18:43.784652 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:18:43.785271 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:18:43.787893 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:18:43.789492 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:18:43.795406 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:18:43.796311 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:18:43.797236 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:18:43.846928 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:43.851580 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:18:43.853658 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:43.906002 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:43.909122 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021771D9A2B0>]}
[0m15:18:43.910070 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:18:43.911326 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:18:43.912057 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:18:43.913123 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:18:43.914231 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:18:43.914850 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:18:43.917805 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:18:43.919485 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:18:43.923364 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:18:43.924152 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:18:43.925540 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:18:43.974594 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:43.977570 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:18:43.979276 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:44.030898 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.034707 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176F3B3C50>]}
[0m15:18:44.035672 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:18:44.036988 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:18:44.037705 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:44.038738 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:18:44.039922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:18:44.040959 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:44.044699 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:18:44.046404 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:44.051166 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:18:44.051975 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:18:44.052773 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:18:44.103743 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.109138 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:18:44.112298 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:44.171893 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:44.177568 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002177214A7D0>]}
[0m15:18:44.179272 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.14s]
[0m15:18:44.181395 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:18:44.183956 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:18:44.186116 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:18:44.187627 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:18:44.188817 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:18:44.197487 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:18:44.201161 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:18:44.205715 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:18:44.206701 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:18:44.208229 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:18:44.259490 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.264499 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:18:44.266218 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:44.318640 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.322138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176F57F230>]}
[0m15:18:44.323377 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.13s]
[0m15:18:44.324902 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:18:44.325664 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:18:44.326679 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:18:44.327862 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:18:44.328558 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:18:44.333194 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:18:44.334862 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:18:44.339388 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:18:44.340289 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:18:44.341161 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:18:44.391110 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.394241 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:18:44.395832 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:44.446208 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.449896 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021772237290>]}
[0m15:18:44.450951 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m15:18:44.452260 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:18:44.452933 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:18:44.454035 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:18:44.455066 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:18:44.455731 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:18:44.459796 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:18:44.461322 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:18:44.465313 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:18:44.466542 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:18:44.467661 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:18:44.515430 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.518488 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:18:44.520191 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:18:44.570084 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.574428 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021772237890>]}
[0m15:18:44.576063 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m15:18:44.577840 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:18:44.578948 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:18:44.580756 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:18:44.582017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:18:44.582684 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:18:44.587127 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:18:44.588604 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:18:44.671486 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:18:44.726056 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.760960 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:44.810630 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.817143 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:18:44.818820 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:18:44.883091 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:44.895125 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:18:44.903002 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:44.913275 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:18:44.960983 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:44.963672 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021772236BD0>]}
[0m15:18:44.964694 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.38s]
[0m15:18:44.965907 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:18:44.966733 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:18:44.967752 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:18:44.968750 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:18:44.969387 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:18:44.973330 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:18:44.974937 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:18:44.981528 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:18:45.038167 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:45.042694 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:45.090690 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.093748 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:18:45.095582 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:18:45.154854 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:45.157236 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:18:45.165150 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:45.170835 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:18:45.217062 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.219694 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021772237830>]}
[0m15:18:45.221040 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.25s]
[0m15:18:45.222797 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:18:45.223694 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:18:45.224652 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:18:45.225649 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:18:45.226264 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:18:45.230720 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:18:45.232709 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:18:45.246395 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:18:45.304284 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:45.308378 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:45.356541 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.361949 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:18:45.364692 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:18:45.424482 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:45.426737 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:18:45.434713 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:45.440841 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:18:45.484811 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:18:45.487329 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217722370B0>]}
[0m15:18:45.488406 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.26s]
[0m15:18:45.489655 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:18:45.490321 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:18:45.491396 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:18:45.492580 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:18:45.493535 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:18:45.499211 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:18:45.500969 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:18:45.507917 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:18:45.562008 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.566206 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:45.619469 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.622253 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:18:45.623906 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:18:45.678702 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.681085 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:18:45.688421 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:45.694593 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:18:45.741271 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.743664 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021772237530>]}
[0m15:18:45.744631 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.25s]
[0m15:18:45.746011 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:18:45.746871 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_point
[0m15:18:45.747916 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_point` ...................... [RUN]
[0m15:18:45.749072 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_point)
[0m15:18:45.749710 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_point
[0m15:18:45.755269 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_point"
[0m15:18:45.756859 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_point
[0m15:18:45.764151 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

            

    
        create table `default`.`dim_stop_point__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:18:45.824206 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:18:45.828456 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

    select name, type from system.columns where table = 'dim_stop_point__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:18:45.874995 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:45.878256 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_point"
[0m15:18:45.880233 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_point__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:18:45.954108 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:18:45.957058 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
EXCHANGE TABLES `default`.`dim_stop_point__dbt_backup` AND `default`.`dim_stop_point` 
  
  ...
[0m15:18:45.965604 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:18:45.970948 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_point: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_point"} */
drop table if exists `default`.`dim_stop_point__dbt_backup` 
  ...
[0m15:18:46.016788 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:18:46.019155 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46c578be-ec2d-4449-a806-9538392b89e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021772237EF0>]}
[0m15:18:46.020117 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_point` ................. [[32mOK[0m in 0.27s]
[0m15:18:46.021337 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_point
[0m15:18:46.023703 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:18:46.024489 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:18:46.025036 [debug] [MainThread]: On list_: Close
[0m15:18:46.025545 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:18:46.026160 [debug] [MainThread]: On list__default: Close
[0m15:18:46.026936 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_point' was left open.
[0m15:18:46.027452 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_point: Close
[0m15:18:46.028434 [info ] [MainThread]: 
[0m15:18:46.029392 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 5.30 seconds (5.30s).
[0m15:18:46.034274 [debug] [MainThread]: Command end result
[0m15:18:46.084718 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:18:46.089357 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:18:46.108075 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:18:46.108948 [info ] [MainThread]: 
[0m15:18:46.110224 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:18:46.111145 [info ] [MainThread]: 
[0m15:18:46.112111 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m15:18:46.114445 [debug] [MainThread]: Command `dbt run` succeeded at 15:18:46.114183 after 8.70 seconds
[0m15:18:46.115100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002176CE064B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217722BF9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217722BFB30>]}
[0m15:18:46.115920 [debug] [MainThread]: Flushing usage events
[0m15:18:46.698611 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:28:56.258082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931F0E6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931FBE1F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193212A7ED0>]}


============================== 15:28:56.265449 | 27692ff9-a2f4-46c3-a784-fc414b88aed9 ==============================
[0m15:28:56.265449 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:28:56.266553 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --full-refresh', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'printer_width': '80', 'write_json': 'True', 'warn_error': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'use_colors': 'True', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m15:28:56.855391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931F5DE190>]}
[0m15:28:56.991418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193211B2AD0>]}
[0m15:28:56.994508 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:28:57.522461 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:28:57.863822 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m15:28:57.864827 [debug] [MainThread]: Partial parsing: added file: dataengineering_dbt://models\gold\dim_stop_place.sql
[0m15:28:57.865320 [debug] [MainThread]: Partial parsing: deleted file: dataengineering_dbt://models\gold\dim_stop_point.sql
[0m15:28:58.493833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193218A1450>]}
[0m15:28:58.700475 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:28:58.706917 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:28:58.756201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931F0DE030>]}
[0m15:28:58.757243 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:28:58.758919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019321512970>]}
[0m15:28:58.767653 [info ] [MainThread]: 
[0m15:28:58.768787 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:28:58.769641 [info ] [MainThread]: 
[0m15:28:58.770676 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:28:58.788141 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:28:58.807060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:29:00.415517 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:29:00.465919 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:00.502874 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:29:00.514261 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:29:01.055139 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:29:01.103003 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:01.112666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019324343AD0>]}
[0m15:29:01.119123 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:29:01.120158 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:29:01.121356 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:29:01.122126 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:29:01.137037 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:29:01.140073 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:29:01.171499 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:29:01.194806 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:29:01.200374 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:29:01.727242 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:29:01.776865 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:01.807161 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:29:01.808999 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:01.860643 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:01.911584 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001932452D790>]}
[0m15:29:01.913230 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.79s]
[0m15:29:01.914591 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:29:01.915273 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:29:01.916552 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:29:01.917769 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:29:01.918383 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:29:01.920983 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:29:01.922537 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:29:01.926676 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:29:01.927941 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:29:01.928950 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:29:01.977023 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:01.981095 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:29:01.983038 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:02.033776 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.036862 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193242E97B0>]}
[0m15:29:02.037829 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:29:02.039093 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:29:02.039777 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:29:02.040950 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:29:02.042188 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:29:02.042901 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:29:02.047416 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:29:02.049558 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:29:02.054718 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:29:02.055889 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:29:02.057892 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:29:02.105574 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.111019 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:29:02.114565 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'Name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:02.168996 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.172138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001932462B2F0>]}
[0m15:29:02.173103 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.13s]
[0m15:29:02.174360 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:29:02.175028 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:29:02.176268 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:29:02.177998 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:29:02.179014 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:29:02.183239 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:29:02.185131 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:29:02.190846 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:29:02.192037 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:29:02.193724 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:29:02.240987 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.245999 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:29:02.248113 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:02.302041 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.305232 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019324670B50>]}
[0m15:29:02.306230 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.13s]
[0m15:29:02.307474 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:29:02.308408 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:29:02.310328 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:29:02.311592 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:29:02.312360 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:29:02.317806 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:29:02.319480 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:29:02.323068 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:29:02.323908 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:29:02.324745 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:29:02.382271 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:02.385386 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:29:02.387038 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:02.436912 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.440186 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019321548C20>]}
[0m15:29:02.441276 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.13s]
[0m15:29:02.442699 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:29:02.443841 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:29:02.445783 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:29:02.447272 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:29:02.447970 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:29:02.452270 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:29:02.453961 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:29:02.458758 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:29:02.460085 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:29:02.462107 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:29:02.509784 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.513909 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:29:02.516041 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:02.568397 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.571477 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193247848F0>]}
[0m15:29:02.572474 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m15:29:02.573748 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:29:02.574399 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:29:02.575395 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:29:02.576878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:29:02.578069 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:29:02.585057 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:29:02.587121 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:29:02.591815 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:29:02.592625 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:29:02.593745 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:29:02.640923 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:02.644772 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:29:02.646436 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:29:02.705268 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:02.710869 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193245D1CD0>]}
[0m15:29:02.712622 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.13s]
[0m15:29:02.715109 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:29:02.716755 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:29:02.718633 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:29:02.720261 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:29:02.721493 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:29:02.732969 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:29:02.735707 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:29:02.844341 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:29:02.902889 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:02.944578 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:29:02.993637 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.000370 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:29:03.002396 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:29:03.069503 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:29:03.078892 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:29:03.087537 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:29:03.098527 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:29:03.143650 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:29:03.146632 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193246E31D0>]}
[0m15:29:03.147733 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.43s]
[0m15:29:03.149120 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:29:03.149867 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:29:03.150938 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:29:03.152166 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:29:03.152857 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:29:03.157218 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:29:03.158840 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:29:03.167553 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:29:03.225417 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:03.229952 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:29:03.277456 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.280515 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:29:03.282030 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:29:03.337930 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.341041 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:29:03.369559 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.03 seconds
[0m15:29:03.375234 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:29:03.419110 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:29:03.421616 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019324785CD0>]}
[0m15:29:03.422631 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.27s]
[0m15:29:03.424075 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:29:03.424918 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:29:03.426388 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:29:03.427460 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:29:03.428097 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:29:03.432259 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:29:03.433927 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:29:03.441161 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:29:03.497319 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:03.503609 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:29:03.552916 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.556522 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:29:03.558209 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:29:03.615515 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:03.617680 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:29:03.625526 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:29:03.630954 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:29:03.675214 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:29:03.678649 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019324785730>]}
[0m15:29:03.680031 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m15:29:03.681400 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:29:03.682042 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:29:03.683095 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:29:03.684235 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:29:03.684895 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:29:03.688759 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:29:03.690363 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:29:03.698270 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:29:03.751900 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.755791 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:29:03.804793 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.807573 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:29:03.808976 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:29:03.865497 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:29:03.867439 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:29:03.874930 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:29:03.880808 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:29:03.927033 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:03.929357 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019324784B90>]}
[0m15:29:03.930308 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.25s]
[0m15:29:03.931528 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:29:03.932261 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m15:29:03.933318 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_place` ...................... [RUN]
[0m15:29:03.934499 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_place)
[0m15:29:03.935151 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m15:29:03.940454 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m15:29:03.942468 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m15:29:03.948598 [debug] [Thread-1 (]: Creating new relation dim_stop_place
[0m15:29:03.950595 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

            

    
        create table `default`.`dim_stop_place`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:29:04.006106 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:04.010025 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

    select name, type from system.columns where table = 'dim_stop_place'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:29:04.060910 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:29:04.064088 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_place"
[0m15:29:04.065782 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_place`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:29:04.140876 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:29:04.145287 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27692ff9-a2f4-46c3-a784-fc414b88aed9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193248500B0>]}
[0m15:29:04.146296 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_place` ................. [[32mOK[0m in 0.21s]
[0m15:29:04.147518 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m15:29:04.149935 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:29:04.150564 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:29:04.151110 [debug] [MainThread]: On list_: Close
[0m15:29:04.151564 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:29:04.152028 [debug] [MainThread]: On list__default: Close
[0m15:29:04.152464 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_place' was left open.
[0m15:29:04.152970 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_place: Close
[0m15:29:04.153980 [info ] [MainThread]: 
[0m15:29:04.154864 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 5.38 seconds (5.38s).
[0m15:29:04.159245 [debug] [MainThread]: Command end result
[0m15:29:04.225966 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:29:04.230659 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:29:04.245670 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:29:04.246634 [info ] [MainThread]: 
[0m15:29:04.247719 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:29:04.248721 [info ] [MainThread]: 
[0m15:29:04.249750 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m15:29:04.251939 [debug] [MainThread]: Command `dbt run` succeeded at 15:29:04.251704 after 8.25 seconds
[0m15:29:04.252550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019321129190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193242BF410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193242BF590>]}
[0m15:29:04.253141 [debug] [MainThread]: Flushing usage events
[0m15:29:04.876892 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:42:43.731314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCCF6B6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD01ADF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD1877ED0>]}


============================== 15:42:43.740880 | 11c5e9a5-4670-4808-8e45-19a1ae7f4264 ==============================
[0m15:42:43.740880 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:42:43.742151 [debug] [MainThread]: running dbt with arguments {'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'cache_selected_only': 'False', 'target_path': 'None', 'no_print': 'None', 'invocation_command': 'dbt run --full-refresh', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m15:42:44.329214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCCFBAE190>]}
[0m15:42:44.460076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD1783020>]}
[0m15:42:44.461969 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:42:45.055150 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:42:45.544085 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:42:45.545666 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\staging\stg_netex_quays.sql
[0m15:42:46.257406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD1E99250>]}
[0m15:42:46.441456 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:42:46.446215 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:42:46.501108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCCF6AE030>]}
[0m15:42:46.501975 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:42:46.502884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD1AF2970>]}
[0m15:42:46.508318 [info ] [MainThread]: 
[0m15:42:46.509314 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:42:46.510086 [info ] [MainThread]: 
[0m15:42:46.511171 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:42:46.530854 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:42:46.549403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:48.929986 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:42:48.975084 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:42:49.007952 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:42:49.018250 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:49.541956 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:42:49.589788 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:49.598172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4913AD0>]}
[0m15:42:49.605015 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:42:49.605937 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:42:49.607158 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:42:49.607769 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:42:49.618069 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:42:49.619745 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:42:49.644199 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:42:49.661716 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:42:49.665320 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:42:50.194336 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:42:50.244572 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.272418 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:42:50.274389 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:50.327011 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.364476 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4AFD790>]}
[0m15:42:50.365618 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.76s]
[0m15:42:50.366874 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:42:50.367571 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:42:50.369100 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:42:50.370220 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:42:50.370844 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:42:50.373606 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:42:50.375664 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:42:50.380008 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:42:50.380839 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:42:50.381640 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:42:50.427600 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.430716 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:42:50.432218 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:50.482948 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.486068 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD48B97B0>]}
[0m15:42:50.487095 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m15:42:50.488339 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:42:50.489009 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:42:50.490082 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:42:50.491301 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:42:50.491984 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:42:50.494499 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:42:50.495967 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:42:50.499298 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:42:50.500230 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:42:50.501616 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:42:50.547750 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.550805 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:42:50.552411 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:50.603301 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.606390 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D825D0>]}
[0m15:42:50.607349 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m15:42:50.608581 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:42:50.609213 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:42:50.609987 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:42:50.611056 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:42:50.612174 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:42:50.616385 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:42:50.618272 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:42:50.623047 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:42:50.623916 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:42:50.624908 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:42:50.671863 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.675623 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:42:50.677669 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:50.730822 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.733808 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD45C0ED0>]}
[0m15:42:50.734717 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.12s]
[0m15:42:50.735914 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:42:50.736534 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:42:50.737549 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:42:50.738789 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:42:50.739442 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:42:50.743254 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:42:50.744828 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:42:50.748156 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:42:50.748954 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:42:50.749726 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:42:50.799635 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.802646 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:42:50.804244 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:50.855658 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.860232 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4DCC360>]}
[0m15:42:50.861637 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.12s]
[0m15:42:50.862947 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:42:50.863558 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:42:50.864731 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:42:50.866022 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:42:50.866663 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:42:50.870357 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:42:50.871845 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:42:50.876397 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:42:50.877201 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:42:50.877956 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:42:50.924285 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.927146 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:42:50.928579 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:50.978818 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:50.981823 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D58710>]}
[0m15:42:50.982725 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m15:42:50.983864 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:42:50.984468 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:42:50.985501 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:42:50.986638 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:42:50.987280 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:42:50.992378 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:42:50.994104 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:42:50.998871 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:42:50.999691 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:42:51.000515 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:42:51.047902 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.051758 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:42:51.053337 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:42:51.102624 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.105949 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4BA18B0>]}
[0m15:42:51.106920 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m15:42:51.108151 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:42:51.108808 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:42:51.110002 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:42:51.111270 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:42:51.111962 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:42:51.116230 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:42:51.117640 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:42:51.223899 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:42:51.280281 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:42:51.309912 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:42:51.356205 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.362205 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:42:51.364262 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:42:51.433030 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m15:42:51.444360 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:42:51.452267 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:42:51.462535 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:42:51.509985 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.512457 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4CBB170>]}
[0m15:42:51.513430 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.40s]
[0m15:42:51.514847 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:42:51.515549 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:42:51.516754 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:42:51.517944 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:42:51.518597 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:42:51.522752 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:42:51.524839 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:42:51.533013 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:42:51.587839 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.592224 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:42:51.640469 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.643389 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:42:51.644888 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:42:51.701656 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:42:51.704091 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:42:51.711556 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:42:51.717361 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:42:51.761934 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m15:42:51.764380 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D58E30>]}
[0m15:42:51.765355 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.25s]
[0m15:42:51.766585 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:42:51.767239 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:42:51.768196 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:42:51.769417 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:42:51.770275 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:42:51.774882 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:42:51.776458 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:42:51.784114 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:42:51.839829 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.844204 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:42:51.892394 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.895872 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:42:51.897472 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:42:51.953292 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:51.955346 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:42:51.962520 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:42:51.970599 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:42:52.018180 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:52.021512 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D5BB30>]}
[0m15:42:52.022872 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m15:42:52.024377 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:42:52.025060 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:42:52.026028 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:42:52.027314 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:42:52.027941 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:42:52.031834 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:42:52.033695 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:42:52.042934 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:42:52.098755 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:52.102722 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:42:52.152276 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:52.156350 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:42:52.157998 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:42:52.214543 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:42:52.216582 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:42:52.224718 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:42:52.230924 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:42:52.278145 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:52.280619 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D58E30>]}
[0m15:42:52.281602 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.25s]
[0m15:42:52.282916 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:42:52.283623 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m15:42:52.284653 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_place` ...................... [RUN]
[0m15:42:52.286190 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_place)
[0m15:42:52.286963 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m15:42:52.294346 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m15:42:52.296358 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m15:42:52.306312 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

            

    
        create table `default`.`dim_stop_place__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:42:52.365147 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:42:52.369514 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

    select name, type from system.columns where table = 'dim_stop_place__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:42:52.415967 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:52.419349 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_place"
[0m15:42:52.421806 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_place__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:42:52.499194 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.08 seconds
[0m15:42:52.501402 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
EXCHANGE TABLES `default`.`dim_stop_place__dbt_backup` AND `default`.`dim_stop_place` 
  
  ...
[0m15:42:52.509847 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:42:52.522032 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
drop table if exists `default`.`dim_stop_place__dbt_backup` 
  ...
[0m15:42:52.570284 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:42:52.574300 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11c5e9a5-4670-4808-8e45-19a1ae7f4264', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D5A930>]}
[0m15:42:52.575957 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_place` ................. [[32mOK[0m in 0.29s]
[0m15:42:52.577595 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m15:42:52.580393 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:42:52.581055 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:42:52.581609 [debug] [MainThread]: On list_: Close
[0m15:42:52.582099 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:42:52.582606 [debug] [MainThread]: On list__default: Close
[0m15:42:52.583267 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_place' was left open.
[0m15:42:52.583810 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_place: Close
[0m15:42:52.584876 [info ] [MainThread]: 
[0m15:42:52.585721 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 6.07 seconds (6.07s).
[0m15:42:52.590629 [debug] [MainThread]: Command end result
[0m15:42:52.646671 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:42:52.651516 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:42:52.669519 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:42:52.670487 [info ] [MainThread]: 
[0m15:42:52.671834 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:42:52.672921 [info ] [MainThread]: 
[0m15:42:52.673867 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m15:42:52.676215 [debug] [MainThread]: Command `dbt run` succeeded at 15:42:52.675952 after 9.24 seconds
[0m15:42:52.676884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD1766C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D59A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCD4D59EB0>]}
[0m15:42:52.677523 [debug] [MainThread]: Flushing usage events
[0m15:42:53.337792 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:53:43.369868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242751E6660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024275CDDF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242773A7ED0>]}


============================== 15:53:43.383151 | cd283459-e0fd-4ef8-8491-871d122bb848 ==============================
[0m15:53:43.383151 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:53:43.384649 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'write_json': 'True', 'use_colors': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'fail_fast': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --full-refresh', 'printer_width': '80', 'target_path': 'None'}
[0m15:53:44.255826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242756DE190>]}
[0m15:53:44.450925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242772B3020>]}
[0m15:53:44.453268 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m15:53:45.145114 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:53:45.661474 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:53:45.662238 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:53:45.795864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242779AD050>]}
[0m15:53:46.133577 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:53:46.142042 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:53:46.214477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242751DE030>]}
[0m15:53:46.215502 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m15:53:46.216923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024277697850>]}
[0m15:53:46.225096 [info ] [MainThread]: 
[0m15:53:46.226322 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:53:46.227329 [info ] [MainThread]: 
[0m15:53:46.228841 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m15:53:46.248824 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m15:53:46.283792 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:49.342664 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m15:53:49.390366 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:49.435605 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m15:53:49.450587 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:49.997097 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m15:53:50.050742 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:50.061450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024277A2F860>]}
[0m15:53:50.069655 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m15:53:50.070760 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m15:53:50.072147 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m15:53:50.073022 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m15:53:50.090861 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:53:50.094841 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m15:53:50.127376 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m15:53:50.163593 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m15:53:50.168527 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:53:50.717358 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m15:53:50.767605 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:50.802095 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m15:53:50.806220 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:50.862032 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:50.919431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A640C50>]}
[0m15:53:50.921313 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.84s]
[0m15:53:50.923342 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m15:53:50.924469 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m15:53:50.926521 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m15:53:50.927954 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m15:53:50.928687 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m15:53:50.932082 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:53:50.933886 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m15:53:50.938483 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m15:53:50.940004 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m15:53:50.941663 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m15:53:50.991667 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:50.995536 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m15:53:50.997723 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:51.051164 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.055991 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A1EBCD0>]}
[0m15:53:51.057743 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.13s]
[0m15:53:51.065612 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m15:53:51.067189 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m15:53:51.068790 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m15:53:51.070763 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m15:53:51.072172 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m15:53:51.077755 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:53:51.080563 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m15:53:51.087098 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m15:53:51.094467 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m15:53:51.097377 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m15:53:51.148519 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.154400 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m15:53:51.157248 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:51.212315 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.216471 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A2F36B0>]}
[0m15:53:51.218445 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.15s]
[0m15:53:51.220139 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m15:53:51.221111 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:53:51.222526 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m15:53:51.224270 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m15:53:51.225351 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:53:51.229274 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:53:51.231239 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:53:51.236299 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m15:53:51.237329 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m15:53:51.238330 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m15:53:51.288660 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.294416 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m15:53:51.297470 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:51.352150 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.356543 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A605650>]}
[0m15:53:51.358330 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.13s]
[0m15:53:51.360345 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m15:53:51.361284 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m15:53:51.363208 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m15:53:51.364454 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m15:53:51.365572 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m15:53:51.372406 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m15:53:51.374575 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m15:53:51.382721 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m15:53:51.384227 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m15:53:51.385854 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m15:53:51.435195 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.440488 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m15:53:51.443339 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:51.496332 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.502517 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A7E4590>]}
[0m15:53:51.504411 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.14s]
[0m15:53:51.506150 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m15:53:51.507181 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m15:53:51.509060 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m15:53:51.510468 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m15:53:51.511302 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m15:53:51.516211 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m15:53:51.517966 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m15:53:51.524541 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m15:53:51.526211 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m15:53:51.528792 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m15:53:51.579794 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.583622 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m15:53:51.585572 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:51.639177 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.643655 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A7774D0>]}
[0m15:53:51.644920 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.13s]
[0m15:53:51.646512 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m15:53:51.647466 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m15:53:51.649200 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m15:53:51.650616 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m15:53:51.651449 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m15:53:51.656349 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m15:53:51.658500 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m15:53:51.665530 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m15:53:51.666569 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m15:53:51.667610 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m15:53:51.722658 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:51.727308 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m15:53:51.729542 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m15:53:51.795601 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:51.799732 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A7754F0>]}
[0m15:53:51.801450 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.15s]
[0m15:53:51.803592 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m15:53:51.804540 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m15:53:51.806217 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m15:53:51.808021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m15:53:51.808993 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m15:53:51.817165 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m15:53:51.821629 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m15:53:51.936403 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m15:53:51.998982 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:52.039257 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:53:52.091984 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:52.103194 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m15:53:52.108324 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m15:53:52.188832 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.08 seconds
[0m15:53:52.201758 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m15:53:52.213253 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:53:52.230376 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m15:53:52.277307 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:52.280540 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A777E30>]}
[0m15:53:52.281829 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.47s]
[0m15:53:52.283422 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m15:53:52.284296 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m15:53:52.285600 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m15:53:52.286848 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m15:53:52.287643 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m15:53:52.293757 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m15:53:52.296473 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m15:53:52.307091 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m15:53:52.370438 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:52.379346 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:53:52.427893 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:52.433388 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m15:53:52.436066 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m15:53:52.499369 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:52.503071 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m15:53:52.512835 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:53:52.520818 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m15:53:52.569175 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:52.572211 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A777B30>]}
[0m15:53:52.573844 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.29s]
[0m15:53:52.575950 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m15:53:52.576876 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m15:53:52.578161 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m15:53:52.579398 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m15:53:52.580155 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m15:53:52.585465 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m15:53:52.587982 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m15:53:52.604454 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m15:53:52.664905 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:52.669929 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:53:52.720394 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:52.724206 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m15:53:52.726514 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m15:53:52.792204 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:52.794871 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m15:53:52.805441 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:53:52.813959 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m15:53:52.861056 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:52.863832 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A777170>]}
[0m15:53:52.864971 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.28s]
[0m15:53:52.866391 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m15:53:52.867203 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m15:53:52.868296 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m15:53:52.869739 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m15:53:52.870546 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m15:53:52.876751 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m15:53:52.879474 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m15:53:52.890208 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m15:53:52.950262 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:52.955841 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:53:53.004996 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:53.010105 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m15:53:53.012621 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m15:53:53.076870 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:53.079699 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m15:53:53.090184 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:53:53.098263 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m15:53:53.145305 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:53.148277 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A776E70>]}
[0m15:53:53.149481 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.28s]
[0m15:53:53.151200 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m15:53:53.152143 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m15:53:53.153536 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_place` ...................... [RUN]
[0m15:53:53.154969 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_place)
[0m15:53:53.155808 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m15:53:53.165834 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m15:53:53.168108 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m15:53:53.176639 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

            

    
        create table `default`.`dim_stop_place__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m15:53:53.237697 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m15:53:53.243142 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

    select name, type from system.columns where table = 'dim_stop_place__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m15:53:53.292423 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:53.296435 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_place"
[0m15:53:53.298981 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_place__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m15:53:53.385302 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.09 seconds
[0m15:53:53.388350 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
EXCHANGE TABLES `default`.`dim_stop_place__dbt_backup` AND `default`.`dim_stop_place` 
  
  ...
[0m15:53:53.398078 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m15:53:53.405179 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
drop table if exists `default`.`dim_stop_place__dbt_backup` 
  ...
[0m15:53:53.453468 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m15:53:53.457948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd283459-e0fd-4ef8-8491-871d122bb848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A7760F0>]}
[0m15:53:53.460011 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_place` ................. [[32mOK[0m in 0.30s]
[0m15:53:53.461953 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m15:53:53.465587 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:53:53.466441 [debug] [MainThread]: Connection 'list_' was left open.
[0m15:53:53.467195 [debug] [MainThread]: On list_: Close
[0m15:53:53.468104 [debug] [MainThread]: Connection 'list__default' was left open.
[0m15:53:53.469224 [debug] [MainThread]: On list__default: Close
[0m15:53:53.470246 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_place' was left open.
[0m15:53:53.471382 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_place: Close
[0m15:53:53.473446 [info ] [MainThread]: 
[0m15:53:53.475074 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 7.24 seconds (7.24s).
[0m15:53:53.484060 [debug] [MainThread]: Command end result
[0m15:53:53.580095 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m15:53:53.587127 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m15:53:53.611943 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m15:53:53.612718 [info ] [MainThread]: 
[0m15:53:53.613801 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:53:53.614847 [info ] [MainThread]: 
[0m15:53:53.615855 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m15:53:53.618460 [debug] [MainThread]: Command `dbt run` succeeded at 15:53:53.618152 after 10.69 seconds
[0m15:53:53.619303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242755D9670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A85F9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002427A85F8F0>]}
[0m15:53:53.620073 [debug] [MainThread]: Flushing usage events
[0m15:53:54.279722 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:01:59.281778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DA786660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DB27DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DC947ED0>]}


============================== 16:01:59.294199 | e31847d7-d73c-4440-8c61-89b24837719a ==============================
[0m16:01:59.294199 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:01:59.295574 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'no_print': 'None', 'write_json': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'invocation_command': 'dbt run --full-refresh', 'printer_width': '80', 'target_path': 'None'}
[0m16:02:00.083050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DAC7E190>]}
[0m16:02:00.286356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DC853020>]}
[0m16:02:00.289692 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m16:02:00.991278 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m16:02:01.631769 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:02:01.632684 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:02:01.770236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DCF31050>]}
[0m16:02:02.040237 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m16:02:02.046753 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m16:02:02.121242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DA77E030>]}
[0m16:02:02.122309 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m16:02:02.123589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DCC77850>]}
[0m16:02:02.129818 [info ] [MainThread]: 
[0m16:02:02.131073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:02:02.132213 [info ] [MainThread]: 
[0m16:02:02.133867 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m16:02:02.157077 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m16:02:02.191023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:04.617545 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m16:02:04.662837 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m16:02:04.694822 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m16:02:04.704343 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:05.221694 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m16:02:05.273668 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:05.281632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DD00B860>]}
[0m16:02:05.289383 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m16:02:05.290308 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m16:02:05.291351 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m16:02:05.291924 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m16:02:05.301641 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m16:02:05.303522 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m16:02:05.326143 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m16:02:05.347597 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m16:02:05.351399 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:02:05.869578 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m16:02:05.916942 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:05.944347 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m16:02:05.946219 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:05.999251 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.035963 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFBECC50>]}
[0m16:02:06.037072 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.74s]
[0m16:02:06.038390 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m16:02:06.039104 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m16:02:06.039994 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m16:02:06.041366 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m16:02:06.042019 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m16:02:06.044646 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m16:02:06.046128 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m16:02:06.050247 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m16:02:06.051059 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m16:02:06.051914 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m16:02:06.099434 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.102632 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m16:02:06.104522 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:06.154860 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.158125 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DF78BCD0>]}
[0m16:02:06.159117 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m16:02:06.160402 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m16:02:06.161041 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m16:02:06.162175 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m16:02:06.163351 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m16:02:06.163979 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m16:02:06.166546 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m16:02:06.168112 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m16:02:06.172347 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m16:02:06.173543 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m16:02:06.174979 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m16:02:06.223339 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.226592 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m16:02:06.228228 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:06.279407 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.282677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DF8936B0>]}
[0m16:02:06.283708 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m16:02:06.285401 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m16:02:06.286581 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:02:06.287973 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m16:02:06.289078 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m16:02:06.289712 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:02:06.292509 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m16:02:06.294134 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:02:06.299755 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m16:02:06.300998 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m16:02:06.302317 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m16:02:06.352910 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.356816 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m16:02:06.358868 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:06.416151 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:06.419925 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DC955ED0>]}
[0m16:02:06.421120 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.13s]
[0m16:02:06.422669 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:02:06.423627 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m16:02:06.425420 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m16:02:06.426467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m16:02:06.427122 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m16:02:06.431472 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m16:02:06.433165 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m16:02:06.439485 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m16:02:06.440717 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m16:02:06.441581 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m16:02:06.496709 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.501755 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m16:02:06.504203 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:06.556030 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.559924 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD5C440>]}
[0m16:02:06.561482 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.13s]
[0m16:02:06.563012 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m16:02:06.563930 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m16:02:06.565344 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m16:02:06.566692 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m16:02:06.567529 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m16:02:06.573857 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m16:02:06.575670 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m16:02:06.579821 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m16:02:06.580751 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m16:02:06.581653 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m16:02:06.633177 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.637998 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m16:02:06.640419 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:06.691266 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.696789 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD174D0>]}
[0m16:02:06.698854 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.13s]
[0m16:02:06.701355 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m16:02:06.703017 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m16:02:06.704793 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m16:02:06.706233 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m16:02:06.707587 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m16:02:06.715183 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m16:02:06.717870 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m16:02:06.723183 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m16:02:06.724244 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m16:02:06.725223 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m16:02:06.772987 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.776469 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m16:02:06.778546 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m16:02:06.830688 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:06.834597 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD16B10>]}
[0m16:02:06.835807 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.13s]
[0m16:02:06.837426 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m16:02:06.838268 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m16:02:06.839639 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m16:02:06.840848 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m16:02:06.841570 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m16:02:06.846756 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m16:02:06.848962 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m16:02:06.958227 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m16:02:07.015885 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:07.048210 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m16:02:07.096244 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.103593 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m16:02:07.105498 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m16:02:07.185257 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.08 seconds
[0m16:02:07.194310 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m16:02:07.203745 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m16:02:07.215308 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m16:02:07.262004 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.266006 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD17EF0>]}
[0m16:02:07.267562 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.43s]
[0m16:02:07.269192 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m16:02:07.269972 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m16:02:07.271178 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m16:02:07.272390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m16:02:07.273072 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m16:02:07.277507 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m16:02:07.278980 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m16:02:07.285665 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m16:02:07.343420 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:07.348320 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m16:02:07.395696 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.399056 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m16:02:07.400773 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m16:02:07.456841 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:07.458919 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m16:02:07.467099 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m16:02:07.473605 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m16:02:07.517540 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m16:02:07.520425 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD17B30>]}
[0m16:02:07.521490 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.25s]
[0m16:02:07.522763 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m16:02:07.523447 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m16:02:07.524438 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m16:02:07.525558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m16:02:07.526241 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m16:02:07.530386 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m16:02:07.531894 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m16:02:07.542280 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m16:02:07.600720 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:07.604773 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m16:02:07.651607 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.654805 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m16:02:07.656456 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m16:02:07.715071 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:07.717134 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m16:02:07.724954 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m16:02:07.730843 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m16:02:07.777444 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.779881 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD16E70>]}
[0m16:02:07.780961 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m16:02:07.782217 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m16:02:07.782900 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m16:02:07.783902 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m16:02:07.785141 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m16:02:07.785833 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m16:02:07.790283 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m16:02:07.792042 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m16:02:07.799571 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m16:02:07.854952 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.859000 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m16:02:07.908428 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:07.911578 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m16:02:07.913799 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m16:02:07.973434 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:07.975641 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m16:02:07.984161 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m16:02:07.990476 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m16:02:08.037798 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:08.040321 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD15190>]}
[0m16:02:08.041323 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.26s]
[0m16:02:08.042580 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m16:02:08.043256 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m16:02:08.044243 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_place` ...................... [RUN]
[0m16:02:08.045479 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_place)
[0m16:02:08.046155 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m16:02:08.051853 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m16:02:08.053700 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m16:02:08.060669 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

            

    
        create table `default`.`dim_stop_place__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m16:02:08.119027 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m16:02:08.124446 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

    select name, type from system.columns where table = 'dim_stop_place__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m16:02:08.172953 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:08.177296 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_place"
[0m16:02:08.180063 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_place__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m16:02:08.280847 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.10 seconds
[0m16:02:08.284112 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
EXCHANGE TABLES `default`.`dim_stop_place__dbt_backup` AND `default`.`dim_stop_place` 
  
  ...
[0m16:02:08.296550 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m16:02:08.303552 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
drop table if exists `default`.`dim_stop_place__dbt_backup` 
  ...
[0m16:02:08.353891 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:02:08.356764 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e31847d7-d73c-4440-8c61-89b24837719a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFD14FB0>]}
[0m16:02:08.358059 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_place` ................. [[32mOK[0m in 0.31s]
[0m16:02:08.359876 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m16:02:08.362623 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:02:08.363303 [debug] [MainThread]: Connection 'list_' was left open.
[0m16:02:08.363893 [debug] [MainThread]: On list_: Close
[0m16:02:08.364422 [debug] [MainThread]: Connection 'list__default' was left open.
[0m16:02:08.364981 [debug] [MainThread]: On list__default: Close
[0m16:02:08.365505 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_place' was left open.
[0m16:02:08.366061 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_place: Close
[0m16:02:08.367214 [info ] [MainThread]: 
[0m16:02:08.368026 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 6.23 seconds (6.23s).
[0m16:02:08.373902 [debug] [MainThread]: Command end result
[0m16:02:08.441265 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m16:02:08.445563 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m16:02:08.468491 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m16:02:08.469275 [info ] [MainThread]: 
[0m16:02:08.470575 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:02:08.471972 [info ] [MainThread]: 
[0m16:02:08.473384 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m16:02:08.475400 [debug] [MainThread]: Command `dbt run` succeeded at 16:02:08.475181 after 9.56 seconds
[0m16:02:08.476071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DAB330B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFDFF9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227DFDFFB30>]}
[0m16:02:08.476738 [debug] [MainThread]: Flushing usage events
[0m16:02:09.104768 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:05:00.618197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FC086660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FCB7DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE247ED0>]}


============================== 16:05:00.628545 | 7938f7d8-7e16-4cf6-82ec-606d620fb10c ==============================
[0m16:05:00.628545 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:05:00.630492 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt docs generate', 'warn_error': 'None', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'partial_parse': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'cache_selected_only': 'False', 'write_json': 'True'}
[0m16:05:01.491211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7938f7d8-7e16-4cf6-82ec-606d620fb10c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FC57E190>]}
[0m16:05:01.784536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7938f7d8-7e16-4cf6-82ec-606d620fb10c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE153570>]}
[0m16:05:01.787490 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m16:05:02.451899 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m16:05:03.119616 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:05:03.120680 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:05:03.289135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7938f7d8-7e16-4cf6-82ec-606d620fb10c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE875150>]}
[0m16:05:03.365304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7938f7d8-7e16-4cf6-82ec-606d620fb10c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE5CA4E0>]}
[0m16:05:03.366732 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m16:05:03.368059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7938f7d8-7e16-4cf6-82ec-606d620fb10c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE5A6890>]}
[0m16:05:03.374259 [info ] [MainThread]: 
[0m16:05:03.375775 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:05:03.377032 [info ] [MainThread]: 
[0m16:05:03.378584 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m16:05:03.401930 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m16:05:03.440843 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:05:05.415589 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m16:05:05.469942 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m16:05:05.512989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7938f7d8-7e16-4cf6-82ec-606d620fb10c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE9176C0>]}
[0m16:05:05.519391 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m16:05:05.520458 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m16:05:05.521179 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m16:05:05.533459 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m16:05:05.535432 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m16:05:05.537676 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m16:05:05.539013 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m16:05:05.540392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m16:05:05.541234 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m16:05:05.545192 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m16:05:05.547118 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m16:05:05.549038 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m16:05:05.549881 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m16:05:05.550824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m16:05:05.551676 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m16:05:05.554842 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m16:05:05.556583 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m16:05:05.558860 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m16:05:05.560353 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:05:05.561570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m16:05:05.562366 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:05:05.565891 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m16:05:05.567844 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:05:05.569928 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m16:05:05.571273 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m16:05:05.572523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m16:05:05.573317 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m16:05:05.579708 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m16:05:05.581608 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m16:05:05.583082 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m16:05:05.583913 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m16:05:05.585054 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m16:05:05.586105 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m16:05:05.590853 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m16:05:05.592595 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m16:05:05.594001 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m16:05:05.595119 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m16:05:05.596426 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m16:05:05.597243 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m16:05:05.602489 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m16:05:05.604098 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m16:05:05.605736 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m16:05:05.606552 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m16:05:05.607485 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m16:05:05.608824 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m16:05:05.617316 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m16:05:05.619505 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m16:05:05.621620 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m16:05:05.622928 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m16:05:05.624099 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m16:05:05.625393 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m16:05:05.630137 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m16:05:05.631931 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m16:05:05.633978 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m16:05:05.635270 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m16:05:05.636425 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m16:05:05.637346 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m16:05:05.642939 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m16:05:05.644886 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m16:05:05.647122 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m16:05:05.648439 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6
[0m16:05:05.649674 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6)
[0m16:05:05.650614 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6
[0m16:05:05.677232 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6"
[0m16:05:05.679970 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6
[0m16:05:05.681638 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6
[0m16:05:05.682660 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376
[0m16:05:05.683798 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_silver_timetable_Ingestion_Date.4a76d4cdc6, now test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376)
[0m16:05:05.684562 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376
[0m16:05:05.692847 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376"
[0m16:05:05.695325 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376
[0m16:05:05.697523 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376
[0m16:05:05.698754 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974
[0m16:05:05.699882 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_silver_timetable_LineId.8c694c9376, now test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974)
[0m16:05:05.700765 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974
[0m16:05:05.708336 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974"
[0m16:05:05.710745 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974
[0m16:05:05.712296 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974
[0m16:05:05.713154 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6
[0m16:05:05.714085 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_silver_timetable_QuayId.d20ae85974, now test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6)
[0m16:05:05.714909 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6
[0m16:05:05.722808 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6"
[0m16:05:05.725041 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6
[0m16:05:05.727225 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6
[0m16:05:05.728064 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m16:05:05.728919 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_silver_timetable_ServiceJourneyId.f45a699fe6, now model.dataengineering_dbt.dim_line)
[0m16:05:05.729640 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m16:05:05.734241 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m16:05:05.735884 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m16:05:05.737417 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m16:05:05.738279 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0
[0m16:05:05.739623 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0)
[0m16:05:05.740939 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0
[0m16:05:05.748751 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0"
[0m16:05:05.751272 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0
[0m16:05:05.753872 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0
[0m16:05:05.754774 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55
[0m16:05:05.755686 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_silver_lines_Ingestion_Date.dd5d294bf0, now test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55)
[0m16:05:05.756425 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55
[0m16:05:05.765751 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55"
[0m16:05:05.768175 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55
[0m16:05:05.769715 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55
[0m16:05:05.770527 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef
[0m16:05:05.771618 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_silver_lines_LineId.545fa13e55, now test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef)
[0m16:05:05.772412 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef
[0m16:05:05.786297 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef"
[0m16:05:05.789058 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef
[0m16:05:05.792409 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef
[0m16:05:05.793434 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m16:05:05.794345 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.unique_silver_lines_LineId.4168fc0cef, now model.dataengineering_dbt.dim_stop_place)
[0m16:05:05.795095 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m16:05:05.807871 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m16:05:05.810774 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m16:05:05.812701 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m16:05:05.814183 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368
[0m16:05:05.815570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_stop_place, now test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368)
[0m16:05:05.816704 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368
[0m16:05:05.827593 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368"
[0m16:05:05.830552 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368
[0m16:05:05.832910 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368
[0m16:05:05.834131 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067
[0m16:05:05.835271 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_dim_quay_quay_id.f45e136368, now test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067)
[0m16:05:05.836053 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067
[0m16:05:05.844253 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067"
[0m16:05:05.846490 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067
[0m16:05:05.848430 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067
[0m16:05:05.849525 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d
[0m16:05:05.850754 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.unique_dim_quay_quay_id.75cb457067, now test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d)
[0m16:05:05.851561 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d
[0m16:05:05.864018 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d"
[0m16:05:05.867131 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d
[0m16:05:05.868664 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d
[0m16:05:05.870157 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c
[0m16:05:05.871468 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_dim_service_journey_service_journey_id.c9bd48005d, now test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c)
[0m16:05:05.872399 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c
[0m16:05:05.883889 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c"
[0m16:05:05.886619 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c
[0m16:05:05.888536 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c
[0m16:05:05.889700 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c
[0m16:05:05.891146 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.unique_dim_service_journey_service_journey_id.604617e29c, now test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c)
[0m16:05:05.892383 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c
[0m16:05:05.903033 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c"
[0m16:05:05.905715 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c
[0m16:05:05.907334 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c
[0m16:05:05.908808 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e
[0m16:05:05.910504 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_fact_movements_event_id.1ffc62742c, now test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e)
[0m16:05:05.912044 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e
[0m16:05:05.923205 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e"
[0m16:05:05.927485 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e
[0m16:05:05.930874 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e
[0m16:05:05.932224 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813
[0m16:05:05.933893 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_fact_movements_quay_id.76df39d09e, now test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813)
[0m16:05:05.935225 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813
[0m16:05:05.951392 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813"
[0m16:05:05.953813 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813
[0m16:05:05.956231 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813
[0m16:05:05.957604 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0
[0m16:05:05.959195 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_fact_movements_service_journey_id.f5d863f813, now test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0)
[0m16:05:05.960454 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0
[0m16:05:05.969902 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0"
[0m16:05:05.972450 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0
[0m16:05:05.975034 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0
[0m16:05:05.976353 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.not_null_dim_line_line_id.870544454a
[0m16:05:05.978007 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.unique_fact_movements_event_id.7000ce52d0, now test.dataengineering_dbt.not_null_dim_line_line_id.870544454a)
[0m16:05:05.979121 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.not_null_dim_line_line_id.870544454a
[0m16:05:05.989963 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.not_null_dim_line_line_id.870544454a"
[0m16:05:05.993547 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.not_null_dim_line_line_id.870544454a
[0m16:05:05.996345 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.not_null_dim_line_line_id.870544454a
[0m16:05:05.997670 [debug] [Thread-1 (]: Began running node test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a
[0m16:05:05.998932 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dataengineering_dbt.not_null_dim_line_line_id.870544454a, now test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a)
[0m16:05:05.999814 [debug] [Thread-1 (]: Began compiling node test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a
[0m16:05:06.012695 [debug] [Thread-1 (]: Writing injected SQL for node "test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a"
[0m16:05:06.015691 [debug] [Thread-1 (]: Began executing node test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a
[0m16:05:06.018092 [debug] [Thread-1 (]: Finished running node test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a
[0m16:05:06.020988 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:05:06.021860 [debug] [MainThread]: Connection 'list__default' was left open.
[0m16:05:06.022804 [debug] [MainThread]: On list__default: Close
[0m16:05:06.023716 [debug] [MainThread]: Connection 'test.dataengineering_dbt.unique_dim_line_line_id.b3816db69a' was properly closed.
[0m16:05:06.036593 [debug] [MainThread]: Command end result
[0m16:05:06.275500 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m16:05:06.281037 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m16:05:06.305479 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m16:05:06.312673 [debug] [MainThread]: Acquiring new clickhouse connection 'generate_catalog'
[0m16:05:06.313667 [info ] [MainThread]: Building catalog
[0m16:05:06.331294 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:05:06.899876 [debug] [MainThread]: dbt_clickhouse adapter: On generate_catalog: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "generate_catalog"} */
select
      '' as table_database,
      columns.database as table_schema,
      columns.table as table_name,
      if(tables.engine not in ('MaterializedView', 'View'), 'table', 'view') as table_type,
      nullIf(tables.comment, '') as table_comment,
      columns.name as column_name,
      columns.position as column_index,
      columns.type as column_type,
      nullIf(columns.comment, '') as column_comment,
      null as table_owner
    from system.columns as columns
    join system.tables as tables on tables.database = columns.database and tables.name = columns.table
    where database != 'system' and
    (columns.database = 'default')
    order by columns.database, columns.table, columns.position...
[0m16:05:06.987433 [debug] [MainThread]: dbt_clickhouse adapter: SQL status: OK in 0.09 seconds
[0m16:05:07.066695 [debug] [MainThread]: Wrote artifact CatalogArtifact to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\catalog.json
[0m16:05:07.213744 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m16:05:07.220833 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m16:05:07.221950 [info ] [MainThread]: Catalog written to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\catalog.json
[0m16:05:07.229726 [debug] [MainThread]: Command `dbt docs generate` succeeded at 16:05:07.229333 after 6.97 seconds
[0m16:05:07.230846 [debug] [MainThread]: Connection 'generate_catalog' was left open.
[0m16:05:07.231880 [debug] [MainThread]: On generate_catalog: Close
[0m16:05:07.233019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000160FE056810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016082413B70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016082413D80>]}
[0m16:05:07.234180 [debug] [MainThread]: Flushing usage events
[0m16:05:07.733641 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:05:18.929230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D45D26660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D46821F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D47EE7ED0>]}


============================== 16:05:18.940258 | 2a8cf48d-033d-4c4d-b6fa-2f5eb77777eb ==============================
[0m16:05:18.940258 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:05:18.941980 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'target_path': 'None', 'warn_error': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'quiet': 'False', 'empty': 'None', 'log_cache_events': 'False', 'debug': 'False', 'no_print': 'None', 'invocation_command': 'dbt docs serve', 'printer_width': '80', 'write_json': 'True'}
[0m16:05:19.504721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a8cf48d-033d-4c4d-b6fa-2f5eb77777eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D4621E190>]}
[0m16:05:19.648181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a8cf48d-033d-4c4d-b6fa-2f5eb77777eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D47DF2BE0>]}
[0m14:19:00.195817 [error] [MainThread]: Encountered an error:

[0m14:19:00.378790 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\cli\requires.py", line 178, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\cli\requires.py", line 128, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\cli\requires.py", line 272, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\cli\requires.py", line 303, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\cli\requires.py", line 350, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\cli\main.py", line 307, in docs_serve
    results = task.run()
  File "C:\Users\marti\anaconda3\Lib\site-packages\dbt\task\docs\serve.py", line 29, in run
    httpd.serve_forever()
    ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\marti\anaconda3\Lib\socketserver.py", line 235, in serve_forever
    ready = selector.select(poll_interval)
  File "C:\Users\marti\anaconda3\Lib\selectors.py", line 314, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
              ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marti\anaconda3\Lib\selectors.py", line 305, in _select
    r, w, x = select.select(r, w, w, timeout)
              ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m14:19:00.402106 [debug] [MainThread]: Command `dbt docs serve` failed at 14:19:00.400945 after 80041.05 seconds
[0m14:19:00.419897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D482B4550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D482B4450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027D48291040>]}
[0m14:19:00.427988 [debug] [MainThread]: Flushing usage events
[0m14:19:01.169440 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:20.525206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012677996660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012678491F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012679B57ED0>]}


============================== 14:19:20.551408 | 52278043-5d30-48ff-9b0c-f4f41ec09469 ==============================
[0m14:19:20.551408 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:19:20.552399 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'write_json': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --full-refresh', 'log_format': 'default', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'use_colors': 'True', 'printer_width': '80', 'target_path': 'None'}
[0m14:19:21.363172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012677E7E190>]}
[0m14:19:21.811076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012679A63020>]}
[0m14:19:21.833994 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:19:22.547607 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:19:22.926215 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:19:22.926817 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:19:23.005467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267A1A1050>]}
[0m14:19:23.152732 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:19:23.157569 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:19:23.383984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267798E030>]}
[0m14:19:23.384727 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:19:23.385563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012679EE7850>]}
[0m14:19:23.389209 [info ] [MainThread]: 
[0m14:19:23.390018 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:19:23.390635 [info ] [MainThread]: 
[0m14:19:23.391572 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:19:23.405706 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:19:23.426250 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:30.583375 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m14:19:30.629263 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:30.687269 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:19:30.695547 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:31.211169 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m14:19:31.262103 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:31.270101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267A27F860>]}
[0m14:19:31.290009 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m14:19:31.290964 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m14:19:31.292430 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m14:19:31.293537 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m14:19:31.304255 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m14:19:31.307240 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m14:19:31.335530 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m14:19:31.363773 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m14:19:31.368962 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:19:31.923209 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m14:19:31.979424 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:32.005015 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m14:19:32.007261 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.058164 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.092895 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CE00C50>]}
[0m14:19:32.094364 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.80s]
[0m14:19:32.095567 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m14:19:32.096265 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m14:19:32.097189 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m14:19:32.098576 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m14:19:32.099197 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m14:19:32.101639 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m14:19:32.103102 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m14:19:32.108908 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m14:19:32.110130 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m14:19:32.111646 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m14:19:32.160914 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.163628 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m14:19:32.165194 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.217225 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.220138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267C9ABCD0>]}
[0m14:19:32.220991 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.12s]
[0m14:19:32.222146 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m14:19:32.222780 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m14:19:32.223868 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m14:19:32.225083 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m14:19:32.225707 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m14:19:32.228669 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m14:19:32.230609 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m14:19:32.234175 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m14:19:32.235044 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m14:19:32.236332 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m14:19:32.284442 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.286987 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m14:19:32.288585 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.341023 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.343760 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CAB36B0>]}
[0m14:19:32.344616 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.12s]
[0m14:19:32.345681 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m14:19:32.346260 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:19:32.346991 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m14:19:32.348037 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m14:19:32.348611 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:19:32.350952 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m14:19:32.352587 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:19:32.355598 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m14:19:32.356308 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m14:19:32.357063 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m14:19:32.404542 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.407604 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m14:19:32.409433 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
--WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.459640 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.462352 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012679B65ED0>]}
[0m14:19:32.463213 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.11s]
[0m14:19:32.464218 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:19:32.464801 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m14:19:32.465748 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m14:19:32.466781 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m14:19:32.467471 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m14:19:32.471080 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m14:19:32.473775 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m14:19:32.477286 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m14:19:32.478322 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m14:19:32.479423 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m14:19:32.530028 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.534330 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m14:19:32.536770 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.592962 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.595888 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CFB08A0>]}
[0m14:19:32.596870 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.13s]
[0m14:19:32.598755 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m14:19:32.599971 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m14:19:32.601433 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m14:19:32.602544 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m14:19:32.603279 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m14:19:32.608135 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m14:19:32.610772 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m14:19:32.616147 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m14:19:32.616920 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m14:19:32.617598 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m14:19:32.664473 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.667048 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m14:19:32.669368 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.720349 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.723150 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF37530>]}
[0m14:19:32.723997 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.12s]
[0m14:19:32.725121 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m14:19:32.725714 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m14:19:32.726622 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m14:19:32.727758 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m14:19:32.728331 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m14:19:32.731724 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m14:19:32.733888 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m14:19:32.736965 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m14:19:32.737684 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m14:19:32.738429 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m14:19:32.785126 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.787750 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m14:19:32.790009 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:19:32.840039 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:32.842999 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF36B70>]}
[0m14:19:32.843868 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.12s]
[0m14:19:32.844975 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m14:19:32.845590 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m14:19:32.846635 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m14:19:32.847743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m14:19:32.848285 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m14:19:32.852100 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m14:19:32.854200 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m14:19:32.920817 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m14:19:32.977479 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:33.001870 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:19:33.049646 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:33.057095 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m14:19:33.059559 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m14:19:33.173068 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.11 seconds
[0m14:19:33.180489 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m14:19:33.187650 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:19:33.201036 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m14:19:33.246924 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:19:33.249909 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF37710>]}
[0m14:19:33.250804 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.40s]
[0m14:19:33.251855 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m14:19:33.252407 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m14:19:33.253095 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m14:19:33.253937 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m14:19:33.254926 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m14:19:33.258368 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m14:19:33.261463 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m14:19:33.269577 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m14:19:33.325809 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:33.329657 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:19:33.377570 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:33.380329 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m14:19:33.382454 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m14:19:33.439100 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:33.440836 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m14:19:33.447472 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:19:33.454350 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m14:19:33.499121 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:19:33.502014 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF35190>]}
[0m14:19:33.503153 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.25s]
[0m14:19:33.504500 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m14:19:33.505297 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m14:19:33.506317 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m14:19:33.507407 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m14:19:33.507949 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m14:19:33.512230 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m14:19:33.514163 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m14:19:33.521843 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m14:19:33.579473 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:33.582963 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:19:33.628752 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:33.632687 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m14:19:33.634740 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m14:19:33.696064 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:33.697991 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m14:19:33.705032 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:19:33.710251 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m14:19:33.754784 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:19:33.756896 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF37E90>]}
[0m14:19:33.757847 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m14:19:33.759104 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m14:19:33.759901 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m14:19:33.760798 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m14:19:33.762073 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m14:19:33.762617 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m14:19:33.765894 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m14:19:33.767386 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m14:19:33.772550 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m14:19:33.835586 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:33.839404 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:19:33.889355 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:19:33.891918 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m14:19:33.893267 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m14:19:33.964143 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m14:19:33.965982 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m14:19:33.973501 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:19:33.978943 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m14:19:34.022789 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:19:34.024963 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF36690>]}
[0m14:19:34.025864 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.26s]
[0m14:19:34.026942 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m14:19:34.027524 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m14:19:34.028213 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_place` ...................... [RUN]
[0m14:19:34.029242 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_place)
[0m14:19:34.029895 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m14:19:34.034777 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m14:19:34.036697 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m14:19:34.041875 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

            

    
        create table `default`.`dim_stop_place__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m14:19:34.099478 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:19:34.102975 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

    select name, type from system.columns where table = 'dim_stop_place__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:19:34.148494 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:19:34.151119 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_place"
[0m14:19:34.152980 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_place__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")-- select
--   row_number() over (order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime) as event_id,
--   sp.stop_point_id,
--   cast(s.AimedArrivalTime as Date) as date_id,
--   sp.name as stop_point_name,
--   sp.short_name as stop_point_short_name,
--   sp.centroid_long,
--   sp.centroid_lat,
--   sp.stop_place_type,
--   sp.parent_stop_place_id,
--   sp.ingestion_date
-- from `default`.`silver_stop_places` as sp
-- left join `default`.`silver_timetable` as s
--   on s.StopPointId = sp.stop_point_id
-- where sp.stop_point_id is not null

select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m14:19:34.249551 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.10 seconds
[0m14:19:34.251380 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
EXCHANGE TABLES `default`.`dim_stop_place__dbt_backup` AND `default`.`dim_stop_place` 
  
  ...
[0m14:19:34.258566 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:19:34.263079 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
drop table if exists `default`.`dim_stop_place__dbt_backup` 
  ...
[0m14:19:34.306929 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:19:34.309978 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52278043-5d30-48ff-9b0c-f4f41ec09469', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267CF34FB0>]}
[0m14:19:34.311158 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_place` ................. [[32mOK[0m in 0.28s]
[0m14:19:34.312301 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m14:19:34.314367 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:19:34.314987 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:19:34.315421 [debug] [MainThread]: On list_: Close
[0m14:19:34.315786 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:19:34.316167 [debug] [MainThread]: On list__default: Close
[0m14:19:34.316521 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_place' was left open.
[0m14:19:34.316900 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_place: Close
[0m14:19:34.317720 [info ] [MainThread]: 
[0m14:19:34.318404 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 10.93 seconds (10.93s).
[0m14:19:34.321896 [debug] [MainThread]: Command end result
[0m14:19:34.364004 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:19:34.367483 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:19:34.380621 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m14:19:34.381309 [info ] [MainThread]: 
[0m14:19:34.382166 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:19:34.382836 [info ] [MainThread]: 
[0m14:19:34.383560 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m14:19:34.385568 [debug] [MainThread]: Command `dbt run` succeeded at 14:19:34.385325 after 14.20 seconds
[0m14:19:34.386134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000126776750D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267D02B9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001267D02BB30>]}
[0m14:19:34.386638 [debug] [MainThread]: Flushing usage events
[0m14:19:35.013197 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:43:06.119865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E109516660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10A00DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10B6D7ED0>]}


============================== 14:43:06.130652 | 0312ad6b-b53d-47f9-8284-062cd48f0039 ==============================
[0m14:43:06.130652 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:43:06.132193 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'invocation_command': 'dbt run --full-refresh', 'no_print': 'None', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt', 'warn_error': 'None', 'indirect_selection': 'eager', 'partial_parse': 'True', 'log_path': 'C:\\Users\\marti\\OneDrive\\Documents\\Andmetehnika (dataeng)\\dataengineering_project\\dbt\\logs', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'quiet': 'False', 'introspect': 'True', 'empty': 'False', 'log_cache_events': 'False', 'debug': 'False', 'log_format': 'default', 'use_colors': 'True', 'printer_width': '80', 'write_json': 'True'}
[0m14:43:06.902740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E109A0E190>]}
[0m14:43:07.045540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10B5E3020>]}
[0m14:43:07.047898 [info ] [MainThread]: Registered adapter: clickhouse=1.9.5
[0m14:43:07.560176 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:43:07.945784 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:43:07.947197 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\gold\dim_stop_place.sql
[0m14:43:07.948330 [debug] [MainThread]: Partial parsing: updated file: dataengineering_dbt://models\staging\stg_netex_stopplaces.sql
[0m14:43:08.560938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10BCDD250>]}
[0m14:43:08.813233 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:43:08.825701 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:43:08.890906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10950E030>]}
[0m14:43:08.892165 [info ] [MainThread]: Found 12 models, 17 data tests, 485 macros
[0m14:43:08.893431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10B8F2970>]}
[0m14:43:08.898605 [info ] [MainThread]: 
[0m14:43:08.899534 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:43:08.900311 [info ] [MainThread]: 
[0m14:43:08.901344 [debug] [MainThread]: Acquiring new clickhouse connection 'master'
[0m14:43:08.920188 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list_'
[0m14:43:08.939583 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:43:11.615398 [debug] [ThreadPool]: dbt_clickhouse adapter: On list_: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list_"} */

    select name from system.databases
  ...
[0m14:43:11.666733 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:11.721822 [debug] [ThreadPool]: Acquiring new clickhouse connection 'list__default'
[0m14:43:11.736583 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:43:12.282529 [debug] [ThreadPool]: dbt_clickhouse adapter: On list__default: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "connection_name": "list__default"} */
select
      t.name as name,
      t.database as schema,
      multiIf(
        engine in ('MaterializedView', 'View'), 'view',
        engine = 'Dictionary', 'dictionary',
        'table'
      ) as type,
      db.engine as db_engine,0 as is_on_cluster
          from system.tables as t join system.databases as db on t.database = db.name
        where schema = 'default'
      

  ...
[0m14:43:12.338040 [debug] [ThreadPool]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:12.346141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10E76BA00>]}
[0m14:43:12.353929 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_live_timetable
[0m14:43:12.354787 [info ] [Thread-1 (]: 1 of 12 START sql view model `default`.`stg_live_timetable` .................... [RUN]
[0m14:43:12.355776 [debug] [Thread-1 (]: Acquiring new clickhouse connection 'model.dataengineering_dbt.stg_live_timetable'
[0m14:43:12.356416 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_live_timetable
[0m14:43:12.366706 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_live_timetable"
[0m14:43:12.369358 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_live_timetable
[0m14:43:12.410004 [debug] [Thread-1 (]: Relation stg_live_timetable already exists, replacing it
[0m14:43:12.448508 [debug] [Thread-1 (]: Model mvs to replace ['stg_live_timetable_mv']
[0m14:43:12.453939 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:43:12.998759 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_live_timetable'
  
  ...
[0m14:43:13.062317 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:13.099150 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_live_timetable"
[0m14:43:13.102124 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_live_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_live_timetable"} */


  create or replace view `default`.`stg_live_timetable` 
  
    
  
  
    
    
  as (
    -- Staging model: SIRI live timetable from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from bronze_live_timetable
where ServiceJourneyId is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:13.155932 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.208138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10E7716D0>]}
[0m14:43:13.210545 [info ] [Thread-1 (]: 1 of 12 OK created sql view model `default`.`stg_live_timetable` ............... [[32mOK[0m in 0.85s]
[0m14:43:13.212558 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_live_timetable
[0m14:43:13.213855 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_lines
[0m14:43:13.215541 [info ] [Thread-1 (]: 2 of 12 START sql view model `default`.`stg_netex_lines` ....................... [RUN]
[0m14:43:13.216951 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_live_timetable, now model.dataengineering_dbt.stg_netex_lines)
[0m14:43:13.217737 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_lines
[0m14:43:13.220636 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_lines"
[0m14:43:13.222708 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_lines
[0m14:43:13.230388 [debug] [Thread-1 (]: Relation stg_netex_lines already exists, replacing it
[0m14:43:13.231746 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_lines_mv']
[0m14:43:13.233132 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_lines'
  
  ...
[0m14:43:13.284028 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.289713 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_lines"
[0m14:43:13.292592 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_lines"} */


  create or replace view `default`.`stg_netex_lines` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx lines from bronze
select
  id,
  -- Use ClickHouse JSON extraction functions instead of parse_json(...) syntax
  -- JSONExtractString returns the string value for the given key
  JSONExtractString(payload, 'Name') as name,
  JSONExtractString(payload, 'PublicCode') as public_code,
  -- Optional route / direction fields (may be absent in some NetEx payloads)
  JSONExtractString(payload, 'RouteRef') as route_id,
  JSONExtractString(payload, 'DirectionRef') as direction,
  Ingestion_Date
from bronze_netex_lines
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:13.347320 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.352859 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10E715700>]}
[0m14:43:13.354222 [info ] [Thread-1 (]: 2 of 12 OK created sql view model `default`.`stg_netex_lines` .................. [[32mOK[0m in 0.14s]
[0m14:43:13.356404 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_lines
[0m14:43:13.358932 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_quays
[0m14:43:13.361159 [info ] [Thread-1 (]: 3 of 12 START sql view model `default`.`stg_netex_quays` ....................... [RUN]
[0m14:43:13.363493 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_lines, now model.dataengineering_dbt.stg_netex_quays)
[0m14:43:13.365036 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_quays
[0m14:43:13.370147 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_quays"
[0m14:43:13.372364 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_quays
[0m14:43:13.381241 [debug] [Thread-1 (]: Relation stg_netex_quays already exists, replacing it
[0m14:43:13.382481 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_quays_mv']
[0m14:43:13.384296 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_quays'
  
  ...
[0m14:43:13.433126 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.436397 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_quays"
[0m14:43:13.438226 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_quays: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_quays"} */


  create or replace view `default`.`stg_netex_quays` 
  
    
  
  
    
    
  as (
    -- Staging model: NetEx quays from bronze
select
  id as QuayId,
  JSONExtractString(payload, 'name') as QuayName,
  JSONExtractString(payload, 'stopPlaceId') as StopPlaceId, --ParentStopPointId,
  -- Location composed from centroid latitude/longitude if present in the payload
  concat(
    coalesce(JSONExtractString(payload, 'Centroid_Lat'), ''),
    ',',
    coalesce(JSONExtractString(payload, 'Centroid_Long'), '')
  ) as Location,
  Ingestion_Date
from bronze_netex_quays
where id is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:13.489452 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.494696 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBDE710>]}
[0m14:43:13.495887 [info ] [Thread-1 (]: 3 of 12 OK created sql view model `default`.`stg_netex_quays` .................. [[32mOK[0m in 0.13s]
[0m14:43:13.497182 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_quays
[0m14:43:13.497954 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:43:13.499128 [info ] [Thread-1 (]: 4 of 12 START sql view model `default`.`stg_netex_stopplaces` .................. [RUN]
[0m14:43:13.500313 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_quays, now model.dataengineering_dbt.stg_netex_stopplaces)
[0m14:43:13.501238 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:43:13.504053 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m14:43:13.505953 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:43:13.512403 [debug] [Thread-1 (]: Relation stg_netex_stopplaces already exists, replacing it
[0m14:43:13.513321 [debug] [Thread-1 (]: Model mvs to replace ['stg_netex_stopplaces_mv']
[0m14:43:13.514200 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.stg_netex_stopplaces'
  
  ...
[0m14:43:13.563612 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.570003 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.stg_netex_stopplaces"
[0m14:43:13.572587 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.stg_netex_stopplaces: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.stg_netex_stopplaces"} */


  create or replace view `default`.`stg_netex_stopplaces` 
  
    
  
  
    
    
  as (
    -- Staging model: NeTEx stop places from bronze
-- Expanded to expose all Bronze columns expected by downstream silver/gold models.
SELECT
  --id,
  JSONExtractString(payload, 'id') AS StopPlaceId,
--  JSONExtractString(payload, 'StopPointId') AS StopPointId,
  JSONExtractString(payload, 'Name') AS Name,
  JSONExtractString(payload, 'ShortName') AS ShortName,
  JSONExtractFloat(payload, 'Centroid_Long') AS Centroid_Long,
  JSONExtractFloat(payload, 'Centroid_Lat') AS Centroid_Lat,
  JSONExtractString(payload, 'StopPlaceType') AS StopPlaceType,
  JSONExtractString(payload, 'ParentStopPlaceId') AS ParentStopPlaceId,
  Ingestion_Date AS Ingestion_Date
FROM bronze_netex_stop_places
WHERE JSONExtractString(payload, 'id') IS NOT NULL
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:13.628113 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.632306 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10BC9FCD0>]}
[0m14:43:13.633676 [info ] [Thread-1 (]: 4 of 12 OK created sql view model `default`.`stg_netex_stopplaces` ............. [[32mOK[0m in 0.13s]
[0m14:43:13.635231 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.stg_netex_stopplaces
[0m14:43:13.636047 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_timetable
[0m14:43:13.637269 [info ] [Thread-1 (]: 5 of 12 START sql view model `default`.`silver_timetable` ...................... [RUN]
[0m14:43:13.638425 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.stg_netex_stopplaces, now model.dataengineering_dbt.silver_timetable)
[0m14:43:13.639131 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_timetable
[0m14:43:13.646540 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_timetable"
[0m14:43:13.649542 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_timetable
[0m14:43:13.656072 [debug] [Thread-1 (]: Relation silver_timetable already exists, replacing it
[0m14:43:13.657788 [debug] [Thread-1 (]: Model mvs to replace ['silver_timetable_mv']
[0m14:43:13.659386 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_timetable'
  
  ...
[0m14:43:13.717125 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:13.720394 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_timetable"
[0m14:43:13.722708 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_timetable: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_timetable"} */


  create or replace view `default`.`silver_timetable` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean timetable with all required fields
select
  ServiceJourneyId,
  QuayId,
  StopPointId,
  cast(AimedArrivalTime as Date) as DateId,
  LineId,
  RouteId,
  DirectionRef,
  VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  ArrivalStatus,
  DepartureStatus,
  Cancellation,
  DepartureBoardingActivity,
  Ingestion_Date
from `default`.`stg_live_timetable`
where ActualArrivalTime is not null
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:13.778596 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.782085 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EC28360>]}
[0m14:43:13.783211 [info ] [Thread-1 (]: 5 of 12 OK created sql view model `default`.`silver_timetable` ................. [[32mOK[0m in 0.14s]
[0m14:43:13.784608 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_timetable
[0m14:43:13.785671 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_lines
[0m14:43:13.787150 [info ] [Thread-1 (]: 6 of 12 START sql view model `default`.`silver_lines` .......................... [RUN]
[0m14:43:13.788541 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_timetable, now model.dataengineering_dbt.silver_lines)
[0m14:43:13.789287 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_lines
[0m14:43:13.795338 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_lines"
[0m14:43:13.797581 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_lines
[0m14:43:13.801195 [debug] [Thread-1 (]: Relation silver_lines already exists, replacing it
[0m14:43:13.802126 [debug] [Thread-1 (]: Model mvs to replace ['silver_lines_mv']
[0m14:43:13.803076 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_lines'
  
  ...
[0m14:43:13.854759 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:13.861206 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_lines"
[0m14:43:13.864071 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_lines: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_lines"} */


  create or replace view `default`.`silver_lines` 
  
    
  
  
    
    
  as (
    -- Silver layer: deduplicate lines with route and direction
select
  id,
  name,
  public_code,
  route_id,
  direction,
  Ingestion_Date
from `default`.`stg_netex_lines`
group by id, name, public_code, route_id, direction, Ingestion_Date
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:13.923977 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:13.932643 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB46B0>]}
[0m14:43:13.934426 [info ] [Thread-1 (]: 6 of 12 OK created sql view model `default`.`silver_lines` ..................... [[32mOK[0m in 0.14s]
[0m14:43:13.936687 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_lines
[0m14:43:13.938491 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.silver_stop_places
[0m14:43:13.940399 [info ] [Thread-1 (]: 7 of 12 START sql view model `default`.`silver_stop_places` .................... [RUN]
[0m14:43:13.943305 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_lines, now model.dataengineering_dbt.silver_stop_places)
[0m14:43:13.945598 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.silver_stop_places
[0m14:43:13.952647 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.silver_stop_places"
[0m14:43:13.956161 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.silver_stop_places
[0m14:43:13.963675 [debug] [Thread-1 (]: Relation silver_stop_places already exists, replacing it
[0m14:43:13.964674 [debug] [Thread-1 (]: Model mvs to replace ['silver_stop_places_mv']
[0m14:43:13.965675 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */

    
    select name
    from system.tables
    where engine = 'MaterializedView'
      and extract(create_table_query, 'TO\\s+([^\\s(]+)') = 'default.silver_stop_places'
  
  ...
[0m14:43:14.014747 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:14.019469 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.silver_stop_places"
[0m14:43:14.021877 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.silver_stop_places: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.silver_stop_places"} */


  create or replace view `default`.`silver_stop_places` 
  
    
  
  
    
    
  as (
    -- Silver layer: clean stop places with all required fields
select
  StopPlaceId,
  --StopPointId,
  Name,
  ShortName,
  Centroid_Long,
  Centroid_Lat,
  StopPlaceType,
  ParentStopPlaceId,
  Ingestion_Date
from `default`.`stg_netex_stopplaces`
    
  )
      
      
                    -- end_of_sql
                    
                    ...
[0m14:43:14.079180 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:14.084621 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EA019D0>]}
[0m14:43:14.086415 [info ] [Thread-1 (]: 7 of 12 OK created sql view model `default`.`silver_stop_places` ............... [[32mOK[0m in 0.14s]
[0m14:43:14.088021 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.silver_stop_places
[0m14:43:14.088885 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_quay
[0m14:43:14.091289 [info ] [Thread-1 (]: 8 of 12 START sql table model `default`.`dim_quay` ............................. [RUN]
[0m14:43:14.094380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.silver_stop_places, now model.dataengineering_dbt.dim_quay)
[0m14:43:14.095614 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_quay
[0m14:43:14.101209 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_quay"
[0m14:43:14.103772 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_quay
[0m14:43:14.199777 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

            

    
        create table `default`.`dim_quay__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
          )
        
        ...
[0m14:43:14.261157 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:14.294770 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

    select name, type from system.columns where table = 'dim_quay__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:43:14.345612 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:14.351677 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_quay"
[0m14:43:14.355628 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */

  
    
    
    
        
         


        insert into `default`.`dim_quay__dbt_backup`
        ("quay_id", "stop_place_id", "quay_name", "location")-- Expanded Quay dimension
-- Use NetEx quays staging model for quay metadata (name, parent stop point, location)
select distinct
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  StopPlaceId as stop_place_id,
  QuayName as quay_name,
  Location as location
  --ParentStopPointId as parent_stop_point_id
from `default`.`stg_netex_quays` q
left join `default`.`silver_timetable` s
  on q.QuayId = s.QuayId
where q.QuayId is not null
  ...
[0m14:43:14.442115 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.09 seconds
[0m14:43:14.450738 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
EXCHANGE TABLES `default`.`dim_quay__dbt_backup` AND `default`.`dim_quay` 
  
  ...
[0m14:43:14.459389 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:43:14.471857 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_quay: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_quay"} */
drop table if exists `default`.`dim_quay__dbt_backup` 
  ...
[0m14:43:14.518905 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:14.521364 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EB179B0>]}
[0m14:43:14.522371 [info ] [Thread-1 (]: 8 of 12 OK created sql table model `default`.`dim_quay` ........................ [[32mOK[0m in 0.43s]
[0m14:43:14.523892 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_quay
[0m14:43:14.525212 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_service_journey
[0m14:43:14.526931 [info ] [Thread-1 (]: 9 of 12 START sql table model `default`.`dim_service_journey` .................. [RUN]
[0m14:43:14.528101 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_quay, now model.dataengineering_dbt.dim_service_journey)
[0m14:43:14.528763 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_service_journey
[0m14:43:14.532686 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_service_journey"
[0m14:43:14.535052 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_service_journey
[0m14:43:14.541800 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

            

    
        create table `default`.`dim_service_journey__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
          )
        
        ...
[0m14:43:14.600886 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:14.604766 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

    select name, type from system.columns where table = 'dim_service_journey__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:43:14.653095 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:14.655914 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_service_journey"
[0m14:43:14.658200 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */

  
    
    
    
        
         


        insert into `default`.`dim_service_journey__dbt_backup`
        ("service_journey_id", "date_id", "direction", "VehicleMode", "line_id", "route_id")-- Expanded ServiceJourney dimension
select distinct
  ServiceJourneyId as service_journey_id,
  cast(AimedArrivalTime as Date) as date_id,
  DirectionRef as direction,
  VehicleMode,
  LineId as line_id,
  RouteId as route_id
from `default`.`silver_timetable`
where ServiceJourneyId is not null
  ...
[0m14:43:14.723897 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:14.726536 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
EXCHANGE TABLES `default`.`dim_service_journey__dbt_backup` AND `default`.`dim_service_journey` 
  
  ...
[0m14:43:14.733770 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:43:14.738887 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_service_journey: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_service_journey"} */
drop table if exists `default`.`dim_service_journey__dbt_backup` 
  ...
[0m14:43:14.786554 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:14.788920 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB4DD0>]}
[0m14:43:14.789832 [info ] [Thread-1 (]: 9 of 12 OK created sql table model `default`.`dim_service_journey` ............. [[32mOK[0m in 0.26s]
[0m14:43:14.791782 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_service_journey
[0m14:43:14.792997 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.fact_movements
[0m14:43:14.794154 [info ] [Thread-1 (]: 10 of 12 START sql table model `default`.`fact_movements` ...................... [RUN]
[0m14:43:14.795350 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_service_journey, now model.dataengineering_dbt.fact_movements)
[0m14:43:14.795960 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.fact_movements
[0m14:43:14.799742 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.fact_movements"
[0m14:43:14.801344 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.fact_movements
[0m14:43:14.810307 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

            

    
        create table `default`.`fact_movements__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
          )
        
        ...
[0m14:43:14.866647 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:14.870342 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

    select name, type from system.columns where table = 'fact_movements__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:43:14.916197 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:14.919494 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.fact_movements"
[0m14:43:14.920989 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */

  
    
    
    
        
         


        insert into `default`.`fact_movements__dbt_backup`
        ("event_id", "service_journey_id", "quay_id", "date_id", "AimedArrivalTime", "ActualArrivalTime", "AimedDepartureTime", "ActualDepartureTime", "Ingestion_Date")-- Expanded star schema fact table for Events
select
  row_number() over (order by ServiceJourneyId, QuayId, AimedArrivalTime) as event_id,
  ServiceJourneyId as service_journey_id,
  QuayId as quay_id,
  --StopPointId as stop_point_id,
  cast(AimedArrivalTime as Date) as date_id,
  --LineId as line_id,
  --RouteId as route_id,
  --DirectionRef as direction,
  --VehicleMode,
  AimedArrivalTime,
  ActualArrivalTime,
  AimedDepartureTime,
  ActualDepartureTime,
  --ArrivalStatus,
  --DepartureStatus,
  --Cancellation,
  --DepartureBoardingActivity,
  Ingestion_Date
from `default`.`silver_timetable`
  ...
[0m14:43:14.987176 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m14:43:14.989038 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
EXCHANGE TABLES `default`.`fact_movements__dbt_backup` AND `default`.`fact_movements` 
  
  ...
[0m14:43:14.996946 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:43:15.001875 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.fact_movements: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.fact_movements"} */
drop table if exists `default`.`fact_movements__dbt_backup` 
  ...
[0m14:43:15.046524 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:43:15.048693 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB6D50>]}
[0m14:43:15.049566 [info ] [Thread-1 (]: 10 of 12 OK created sql table model `default`.`fact_movements` ................. [[32mOK[0m in 0.25s]
[0m14:43:15.050697 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.fact_movements
[0m14:43:15.051295 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_line
[0m14:43:15.052162 [info ] [Thread-1 (]: 11 of 12 START sql table model `default`.`dim_line` ............................ [RUN]
[0m14:43:15.053268 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.fact_movements, now model.dataengineering_dbt.dim_line)
[0m14:43:15.053882 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_line
[0m14:43:15.059092 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_line"
[0m14:43:15.060783 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_line
[0m14:43:15.066288 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

            

    
        create table `default`.`dim_line__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            -- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
          )
        
        ...
[0m14:43:15.127536 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:15.131152 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

    select name, type from system.columns where table = 'dim_line__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:43:15.180459 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:15.182957 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_line"
[0m14:43:15.184304 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */

  
    
    
    
        
         


        insert into `default`.`dim_line__dbt_backup`
        ("line_id", "name", "public_code", "route_id", "direction")-- Expanded Line dimension
select
  id as line_id,
  name,
  public_code,
  route_id,
  direction
from `default`.`silver_lines`
  ...
[0m14:43:15.257245 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.07 seconds
[0m14:43:15.260049 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
EXCHANGE TABLES `default`.`dim_line__dbt_backup` AND `default`.`dim_line` 
  
  ...
[0m14:43:15.268486 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:43:15.277641 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_line: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_line"} */
drop table if exists `default`.`dim_line__dbt_backup` 
  ...
[0m14:43:15.322482 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:43:15.326321 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB7B30>]}
[0m14:43:15.327275 [info ] [Thread-1 (]: 11 of 12 OK created sql table model `default`.`dim_line` ....................... [[32mOK[0m in 0.27s]
[0m14:43:15.328454 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_line
[0m14:43:15.329063 [debug] [Thread-1 (]: Began running node model.dataengineering_dbt.dim_stop_place
[0m14:43:15.330076 [info ] [Thread-1 (]: 12 of 12 START sql table model `default`.`dim_stop_place` ...................... [RUN]
[0m14:43:15.331182 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dataengineering_dbt.dim_line, now model.dataengineering_dbt.dim_stop_place)
[0m14:43:15.331774 [debug] [Thread-1 (]: Began compiling node model.dataengineering_dbt.dim_stop_place
[0m14:43:15.335754 [debug] [Thread-1 (]: Writing injected SQL for node "model.dataengineering_dbt.dim_stop_place"
[0m14:43:15.337166 [debug] [Thread-1 (]: Began executing node model.dataengineering_dbt.dim_stop_place
[0m14:43:15.343823 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

            

    
        create table `default`.`dim_stop_place__dbt_backup`
        
  
        
  engine = MergeTree()
        
      order by (tuple())
        
        
        
        
                    -- end_of_sql
                    SETTINGS replicated_deduplication_window='0'

                    
            empty
          as (
            select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
          )
        
        ...
[0m14:43:15.406502 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.06 seconds
[0m14:43:15.411662 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

    select name, type from system.columns where table = 'dim_stop_place__dbt_backup'
    
      
        and database = 'default'
      
    
    order by position
  ...
[0m14:43:15.462603 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.05 seconds
[0m14:43:15.466760 [debug] [Thread-1 (]: Writing runtime sql for node "model.dataengineering_dbt.dim_stop_place"
[0m14:43:15.468712 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */

  
    
    
    
        
         


        insert into `default`.`dim_stop_place__dbt_backup`
        ("event_id", "stop_place_id", "date_id", "stop_point_name", "stop_point_short_name", "Centroid_Long", "Centroid_Lat", "StopPlaceType", "ParentStopPlaceId", "Ingestion_Date")select
  row_number() over (
      order by s.ServiceJourneyId, s.QuayId, s.AimedArrivalTime
  ) as event_id,
  sp.StopPlaceId as stop_place_id,
  --sp.StopPointId as stop_point_id,
  cast(s.AimedArrivalTime as Date) as date_id,
  sp.Name as stop_point_name,
  sp.ShortName as stop_point_short_name,
  sp.Centroid_Long,
  sp.Centroid_Lat,
  sp.StopPlaceType,
  sp.ParentStopPlaceId,
  sp.Ingestion_Date
from `default`.`silver_stop_places` as sp
left join `default`.`silver_timetable` as s
  --on s.StopPointId = sp.StopPointId
  on s.StopPointId = sp.StopPlaceId
where sp.StopPlaceId is not null
  ...
[0m14:43:15.557251 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.09 seconds
[0m14:43:15.560235 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
EXCHANGE TABLES `default`.`dim_stop_place__dbt_backup` AND `default`.`dim_stop_place` 
  
  ...
[0m14:43:15.567518 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.01 seconds
[0m14:43:15.578231 [debug] [Thread-1 (]: dbt_clickhouse adapter: On model.dataengineering_dbt.dim_stop_place: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "dataengineering_dbt", "target_name": "dev", "node_id": "model.dataengineering_dbt.dim_stop_place"} */
drop table if exists `default`.`dim_stop_place__dbt_backup` 
  ...
[0m14:43:15.622520 [debug] [Thread-1 (]: dbt_clickhouse adapter: SQL status: OK in 0.04 seconds
[0m14:43:15.626340 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0312ad6b-b53d-47f9-8284-062cd48f0039', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB7B30>]}
[0m14:43:15.627381 [info ] [Thread-1 (]: 12 of 12 OK created sql table model `default`.`dim_stop_place` ................. [[32mOK[0m in 0.30s]
[0m14:43:15.628721 [debug] [Thread-1 (]: Finished running node model.dataengineering_dbt.dim_stop_place
[0m14:43:15.631284 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:43:15.631857 [debug] [MainThread]: Connection 'list_' was left open.
[0m14:43:15.632329 [debug] [MainThread]: On list_: Close
[0m14:43:15.632762 [debug] [MainThread]: Connection 'list__default' was left open.
[0m14:43:15.633205 [debug] [MainThread]: On list__default: Close
[0m14:43:15.633610 [debug] [MainThread]: Connection 'model.dataengineering_dbt.dim_stop_place' was left open.
[0m14:43:15.634289 [debug] [MainThread]: On model.dataengineering_dbt.dim_stop_place: Close
[0m14:43:15.635271 [info ] [MainThread]: 
[0m14:43:15.635962 [info ] [MainThread]: Finished running 5 table models, 7 view models in 0 hours 0 minutes and 6.73 seconds (6.73s).
[0m14:43:15.640262 [debug] [MainThread]: Command end result
[0m14:43:15.691749 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\manifest.json
[0m14:43:15.696358 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\semantic_manifest.json
[0m14:43:15.713067 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\marti\OneDrive\Documents\Andmetehnika (dataeng)\dataengineering_project\dbt\target\run_results.json
[0m14:43:15.713659 [info ] [MainThread]: 
[0m14:43:15.714529 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:43:15.715282 [info ] [MainThread]: 
[0m14:43:15.716057 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m14:43:15.718237 [debug] [MainThread]: Command `dbt run` succeeded at 14:43:15.717918 after 9.93 seconds
[0m14:43:15.718874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E1098C3710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB5A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E10EBB46B0>]}
[0m14:43:15.719453 [debug] [MainThread]: Flushing usage events
[0m14:43:16.435178 [debug] [MainThread]: An error was encountered while trying to flush usage events
